{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Six_TL_Architecture_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "--xH_tlyn_at",
        "bvpllAyooFoB",
        "sd2e4WhUoKAS",
        "4wKJ-fgAoORp",
        "QZ4jBLCvoUHK",
        "evW_XJRSoYiD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--xH_tlyn_at"
      },
      "source": [
        "# ***VGG16***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5MAHDGNmmLr"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44iItLagYBYp"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8su4W6Vd6ymH"
      },
      "source": [
        "training_data = np.load('/content/drive/MyDrive/####Online_Journal/Total_Left_data_100_update.npy', allow_pickle=True)\n",
        "x_train=np.array([i[0] for i in training_data])\n",
        "y_train=np.array([i[1] for i in training_data])\n",
        "x_train=x_train/255\n",
        "\n",
        "testing_data = np.load('/content/drive/MyDrive/####Online_Journal/Total_Right_data_100_update.npy', allow_pickle=True)\n",
        "x_test=np.array([i[0] for i in testing_data])\n",
        "y_test=np.array([i[1] for i in testing_data])\n",
        "x_test=x_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC7nz7wc61oz"
      },
      "source": [
        "x_train = np.concatenate((x_train, x_test))\n",
        "y_train = np.concatenate((y_train, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOhl1cjg61rk"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v8HV8Lo66ob",
        "outputId": "cefaafea-9ca4-41da-db94-b122d5addd2d"
      },
      "source": [
        "num_classes = 2\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "model = VGG16(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "#model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "  #print(layer ,layer.trainable)\n",
        "\n",
        "out = Dense(2, activation='softmax', name='output')(model.get_layer('fc2').output)\n",
        "\n",
        "\n",
        "vgg_model = Model(image_input, out)\n",
        "\n",
        "vgg_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 4s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 134,268,738\n",
            "Trainable params: 8,194\n",
            "Non-trainable params: 134,260,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BlLC0Q967Iq",
        "outputId": "80b47108-7c5c-4ebb-ac34-17502497e3cd"
      },
      "source": [
        "vgg_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n",
        "history = vgg_model.fit(x_train, y_train, epochs=150, validation_data=(x_test, y_test), batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "38/38 [==============================] - 51s 502ms/step - loss: 0.6930 - accuracy: 0.5242 - val_loss: 0.7014 - val_accuracy: 0.4900\n",
            "Epoch 2/150\n",
            "38/38 [==============================] - 12s 325ms/step - loss: 0.6944 - accuracy: 0.5150 - val_loss: 0.7093 - val_accuracy: 0.4975\n",
            "Epoch 3/150\n",
            "38/38 [==============================] - 12s 330ms/step - loss: 0.6905 - accuracy: 0.5486 - val_loss: 0.7119 - val_accuracy: 0.5075\n",
            "Epoch 4/150\n",
            "38/38 [==============================] - 13s 334ms/step - loss: 0.6870 - accuracy: 0.5316 - val_loss: 0.6840 - val_accuracy: 0.5138\n",
            "Epoch 5/150\n",
            "38/38 [==============================] - 13s 333ms/step - loss: 0.6803 - accuracy: 0.5641 - val_loss: 0.6968 - val_accuracy: 0.5562\n",
            "Epoch 6/150\n",
            "38/38 [==============================] - 13s 337ms/step - loss: 0.6803 - accuracy: 0.5710 - val_loss: 0.6906 - val_accuracy: 0.5688\n",
            "Epoch 7/150\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.6714 - accuracy: 0.5814 - val_loss: 0.6810 - val_accuracy: 0.5350\n",
            "Epoch 8/150\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.6714 - accuracy: 0.5745 - val_loss: 0.6806 - val_accuracy: 0.5425\n",
            "Epoch 9/150\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.6691 - accuracy: 0.5686 - val_loss: 0.6947 - val_accuracy: 0.5638\n",
            "Epoch 10/150\n",
            "38/38 [==============================] - 13s 344ms/step - loss: 0.6689 - accuracy: 0.5775 - val_loss: 0.6944 - val_accuracy: 0.5688\n",
            "Epoch 11/150\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.6715 - accuracy: 0.5805 - val_loss: 0.6800 - val_accuracy: 0.5263\n",
            "Epoch 12/150\n",
            "38/38 [==============================] - 13s 350ms/step - loss: 0.6636 - accuracy: 0.5933 - val_loss: 0.6797 - val_accuracy: 0.5387\n",
            "Epoch 13/150\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.6644 - accuracy: 0.5919 - val_loss: 0.6900 - val_accuracy: 0.5675\n",
            "Epoch 14/150\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.6733 - accuracy: 0.5393 - val_loss: 0.6887 - val_accuracy: 0.5650\n",
            "Epoch 15/150\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.6586 - accuracy: 0.5961 - val_loss: 0.6814 - val_accuracy: 0.5612\n",
            "Epoch 16/150\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.6605 - accuracy: 0.5960 - val_loss: 0.6794 - val_accuracy: 0.5412\n",
            "Epoch 17/150\n",
            "38/38 [==============================] - 13s 357ms/step - loss: 0.6663 - accuracy: 0.5630 - val_loss: 0.6888 - val_accuracy: 0.5213\n",
            "Epoch 18/150\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.6560 - accuracy: 0.5944 - val_loss: 0.6836 - val_accuracy: 0.5263\n",
            "Epoch 19/150\n",
            "38/38 [==============================] - 14s 359ms/step - loss: 0.6645 - accuracy: 0.5765 - val_loss: 0.6792 - val_accuracy: 0.5387\n",
            "Epoch 20/150\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.6659 - accuracy: 0.5606 - val_loss: 0.6855 - val_accuracy: 0.5550\n",
            "Epoch 21/150\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.6628 - accuracy: 0.5747 - val_loss: 0.6820 - val_accuracy: 0.5575\n",
            "Epoch 22/150\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.6577 - accuracy: 0.5916 - val_loss: 0.6791 - val_accuracy: 0.5475\n",
            "Epoch 23/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6585 - accuracy: 0.5811 - val_loss: 0.6804 - val_accuracy: 0.5300\n",
            "Epoch 24/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6570 - accuracy: 0.6125 - val_loss: 0.6874 - val_accuracy: 0.5663\n",
            "Epoch 25/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6547 - accuracy: 0.5885 - val_loss: 0.6815 - val_accuracy: 0.5312\n",
            "Epoch 26/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6686 - accuracy: 0.5662 - val_loss: 0.6789 - val_accuracy: 0.5512\n",
            "Epoch 27/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6553 - accuracy: 0.5815 - val_loss: 0.7038 - val_accuracy: 0.5600\n",
            "Epoch 28/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6692 - accuracy: 0.5774 - val_loss: 0.6819 - val_accuracy: 0.5337\n",
            "Epoch 29/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6637 - accuracy: 0.5781 - val_loss: 0.6863 - val_accuracy: 0.5575\n",
            "Epoch 30/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6578 - accuracy: 0.5852 - val_loss: 0.6789 - val_accuracy: 0.5500\n",
            "Epoch 31/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6571 - accuracy: 0.5938 - val_loss: 0.6788 - val_accuracy: 0.5425\n",
            "Epoch 32/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6664 - accuracy: 0.5781 - val_loss: 0.6950 - val_accuracy: 0.5625\n",
            "Epoch 33/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6587 - accuracy: 0.5788 - val_loss: 0.6792 - val_accuracy: 0.5525\n",
            "Epoch 34/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6656 - accuracy: 0.5700 - val_loss: 0.6864 - val_accuracy: 0.5587\n",
            "Epoch 35/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6611 - accuracy: 0.5945 - val_loss: 0.6826 - val_accuracy: 0.5587\n",
            "Epoch 36/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6545 - accuracy: 0.5764 - val_loss: 0.6950 - val_accuracy: 0.5600\n",
            "Epoch 37/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6574 - accuracy: 0.5931 - val_loss: 0.6781 - val_accuracy: 0.5537\n",
            "Epoch 38/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6494 - accuracy: 0.6007 - val_loss: 0.6787 - val_accuracy: 0.5525\n",
            "Epoch 39/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6533 - accuracy: 0.5916 - val_loss: 0.6789 - val_accuracy: 0.5500\n",
            "Epoch 40/150\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.6616 - accuracy: 0.5883 - val_loss: 0.6909 - val_accuracy: 0.5663\n",
            "Epoch 41/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6595 - accuracy: 0.5882 - val_loss: 0.6780 - val_accuracy: 0.5475\n",
            "Epoch 42/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6483 - accuracy: 0.5911 - val_loss: 0.6798 - val_accuracy: 0.5512\n",
            "Epoch 43/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6512 - accuracy: 0.6025 - val_loss: 0.6775 - val_accuracy: 0.5537\n",
            "Epoch 44/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6574 - accuracy: 0.5747 - val_loss: 0.6782 - val_accuracy: 0.5512\n",
            "Epoch 45/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6547 - accuracy: 0.5838 - val_loss: 0.6902 - val_accuracy: 0.5663\n",
            "Epoch 46/150\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.6498 - accuracy: 0.6009 - val_loss: 0.6787 - val_accuracy: 0.5550\n",
            "Epoch 47/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6507 - accuracy: 0.5914 - val_loss: 0.6809 - val_accuracy: 0.5650\n",
            "Epoch 48/150\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.6499 - accuracy: 0.5991 - val_loss: 0.6770 - val_accuracy: 0.5475\n",
            "Epoch 49/150\n",
            "38/38 [==============================] - 14s 370ms/step - loss: 0.6557 - accuracy: 0.5945 - val_loss: 0.6879 - val_accuracy: 0.5337\n",
            "Epoch 50/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6587 - accuracy: 0.5744 - val_loss: 0.6778 - val_accuracy: 0.5537\n",
            "Epoch 51/150\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.6617 - accuracy: 0.5916 - val_loss: 0.6831 - val_accuracy: 0.5437\n",
            "Epoch 52/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6487 - accuracy: 0.5981 - val_loss: 0.6776 - val_accuracy: 0.5550\n",
            "Epoch 53/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6473 - accuracy: 0.5996 - val_loss: 0.6770 - val_accuracy: 0.5475\n",
            "Epoch 54/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6503 - accuracy: 0.5991 - val_loss: 0.6767 - val_accuracy: 0.5512\n",
            "Epoch 55/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6527 - accuracy: 0.5904 - val_loss: 0.6804 - val_accuracy: 0.5562\n",
            "Epoch 56/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6467 - accuracy: 0.6013 - val_loss: 0.6764 - val_accuracy: 0.5550\n",
            "Epoch 57/150\n",
            "38/38 [==============================] - 14s 370ms/step - loss: 0.6527 - accuracy: 0.5972 - val_loss: 0.6785 - val_accuracy: 0.5550\n",
            "Epoch 58/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6508 - accuracy: 0.6095 - val_loss: 0.6850 - val_accuracy: 0.5700\n",
            "Epoch 59/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6577 - accuracy: 0.5889 - val_loss: 0.6772 - val_accuracy: 0.5550\n",
            "Epoch 60/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6407 - accuracy: 0.6155 - val_loss: 0.6758 - val_accuracy: 0.5450\n",
            "Epoch 61/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6545 - accuracy: 0.5813 - val_loss: 0.6815 - val_accuracy: 0.5638\n",
            "Epoch 62/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6497 - accuracy: 0.5889 - val_loss: 0.6754 - val_accuracy: 0.5512\n",
            "Epoch 63/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6413 - accuracy: 0.6026 - val_loss: 0.6757 - val_accuracy: 0.5575\n",
            "Epoch 64/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6580 - accuracy: 0.5831 - val_loss: 0.6752 - val_accuracy: 0.5525\n",
            "Epoch 65/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6529 - accuracy: 0.5793 - val_loss: 0.6807 - val_accuracy: 0.5650\n",
            "Epoch 66/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6497 - accuracy: 0.5889 - val_loss: 0.6820 - val_accuracy: 0.5688\n",
            "Epoch 67/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6504 - accuracy: 0.5962 - val_loss: 0.6823 - val_accuracy: 0.5713\n",
            "Epoch 68/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6486 - accuracy: 0.6051 - val_loss: 0.6856 - val_accuracy: 0.5688\n",
            "Epoch 69/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6481 - accuracy: 0.6068 - val_loss: 0.6754 - val_accuracy: 0.5550\n",
            "Epoch 70/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6351 - accuracy: 0.6267 - val_loss: 0.6756 - val_accuracy: 0.5537\n",
            "Epoch 71/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6470 - accuracy: 0.5982 - val_loss: 0.6765 - val_accuracy: 0.5663\n",
            "Epoch 72/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6477 - accuracy: 0.5899 - val_loss: 0.6743 - val_accuracy: 0.5512\n",
            "Epoch 73/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6511 - accuracy: 0.5820 - val_loss: 0.6775 - val_accuracy: 0.5600\n",
            "Epoch 74/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6493 - accuracy: 0.5896 - val_loss: 0.6799 - val_accuracy: 0.5700\n",
            "Epoch 75/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6515 - accuracy: 0.6043 - val_loss: 0.6743 - val_accuracy: 0.5487\n",
            "Epoch 76/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6452 - accuracy: 0.5997 - val_loss: 0.6741 - val_accuracy: 0.5512\n",
            "Epoch 77/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6458 - accuracy: 0.6002 - val_loss: 0.6816 - val_accuracy: 0.5713\n",
            "Epoch 78/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6593 - accuracy: 0.5812 - val_loss: 0.6872 - val_accuracy: 0.5688\n",
            "Epoch 79/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6560 - accuracy: 0.5893 - val_loss: 0.6801 - val_accuracy: 0.5725\n",
            "Epoch 80/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6555 - accuracy: 0.6046 - val_loss: 0.6738 - val_accuracy: 0.5487\n",
            "Epoch 81/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6449 - accuracy: 0.6156 - val_loss: 0.6740 - val_accuracy: 0.5612\n",
            "Epoch 82/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6522 - accuracy: 0.5841 - val_loss: 0.6808 - val_accuracy: 0.5725\n",
            "Epoch 83/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6464 - accuracy: 0.6106 - val_loss: 0.6734 - val_accuracy: 0.5525\n",
            "Epoch 84/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6556 - accuracy: 0.5772 - val_loss: 0.6795 - val_accuracy: 0.5600\n",
            "Epoch 85/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6516 - accuracy: 0.5905 - val_loss: 0.6735 - val_accuracy: 0.5562\n",
            "Epoch 86/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6511 - accuracy: 0.6008 - val_loss: 0.6746 - val_accuracy: 0.5612\n",
            "Epoch 87/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6469 - accuracy: 0.5963 - val_loss: 0.6728 - val_accuracy: 0.5462\n",
            "Epoch 88/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6326 - accuracy: 0.6309 - val_loss: 0.6772 - val_accuracy: 0.5650\n",
            "Epoch 89/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6489 - accuracy: 0.6039 - val_loss: 0.6729 - val_accuracy: 0.5462\n",
            "Epoch 90/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6444 - accuracy: 0.6061 - val_loss: 0.6753 - val_accuracy: 0.5562\n",
            "Epoch 91/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6360 - accuracy: 0.6374 - val_loss: 0.6727 - val_accuracy: 0.5487\n",
            "Epoch 92/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6405 - accuracy: 0.6023 - val_loss: 0.6735 - val_accuracy: 0.5725\n",
            "Epoch 93/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6489 - accuracy: 0.6113 - val_loss: 0.6738 - val_accuracy: 0.5788\n",
            "Epoch 94/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6469 - accuracy: 0.5971 - val_loss: 0.6729 - val_accuracy: 0.5562\n",
            "Epoch 95/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6439 - accuracy: 0.6157 - val_loss: 0.6739 - val_accuracy: 0.5587\n",
            "Epoch 96/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6475 - accuracy: 0.5884 - val_loss: 0.6719 - val_accuracy: 0.5512\n",
            "Epoch 97/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6469 - accuracy: 0.5922 - val_loss: 0.6780 - val_accuracy: 0.5725\n",
            "Epoch 98/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6418 - accuracy: 0.6062 - val_loss: 0.6794 - val_accuracy: 0.5750\n",
            "Epoch 99/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6430 - accuracy: 0.6117 - val_loss: 0.6743 - val_accuracy: 0.5800\n",
            "Epoch 100/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6422 - accuracy: 0.6172 - val_loss: 0.6748 - val_accuracy: 0.5763\n",
            "Epoch 101/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6517 - accuracy: 0.6086 - val_loss: 0.6715 - val_accuracy: 0.5550\n",
            "Epoch 102/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6513 - accuracy: 0.5935 - val_loss: 0.6714 - val_accuracy: 0.5537\n",
            "Epoch 103/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6489 - accuracy: 0.5997 - val_loss: 0.6770 - val_accuracy: 0.5587\n",
            "Epoch 104/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6368 - accuracy: 0.6191 - val_loss: 0.6716 - val_accuracy: 0.5587\n",
            "Epoch 105/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6379 - accuracy: 0.6174 - val_loss: 0.6717 - val_accuracy: 0.5638\n",
            "Epoch 106/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6473 - accuracy: 0.5969 - val_loss: 0.6731 - val_accuracy: 0.5612\n",
            "Epoch 107/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6311 - accuracy: 0.6335 - val_loss: 0.6708 - val_accuracy: 0.5487\n",
            "Epoch 108/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6410 - accuracy: 0.6272 - val_loss: 0.7156 - val_accuracy: 0.5650\n",
            "Epoch 109/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6523 - accuracy: 0.5987 - val_loss: 0.6710 - val_accuracy: 0.5575\n",
            "Epoch 110/150\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.6448 - accuracy: 0.6013 - val_loss: 0.6709 - val_accuracy: 0.5562\n",
            "Epoch 111/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6482 - accuracy: 0.6004 - val_loss: 0.6956 - val_accuracy: 0.5763\n",
            "Epoch 112/150\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.6539 - accuracy: 0.5995 - val_loss: 0.6708 - val_accuracy: 0.5625\n",
            "Epoch 113/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6426 - accuracy: 0.6016 - val_loss: 0.6713 - val_accuracy: 0.5700\n",
            "Epoch 114/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6419 - accuracy: 0.6100 - val_loss: 0.6726 - val_accuracy: 0.5788\n",
            "Epoch 115/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6458 - accuracy: 0.5936 - val_loss: 0.6703 - val_accuracy: 0.5537\n",
            "Epoch 116/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6352 - accuracy: 0.6199 - val_loss: 0.6714 - val_accuracy: 0.5775\n",
            "Epoch 117/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6432 - accuracy: 0.6060 - val_loss: 0.6708 - val_accuracy: 0.5612\n",
            "Epoch 118/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6491 - accuracy: 0.5946 - val_loss: 0.6772 - val_accuracy: 0.5775\n",
            "Epoch 119/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6382 - accuracy: 0.6211 - val_loss: 0.6707 - val_accuracy: 0.5688\n",
            "Epoch 120/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6391 - accuracy: 0.6151 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
            "Epoch 121/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6418 - accuracy: 0.6138 - val_loss: 0.6814 - val_accuracy: 0.5788\n",
            "Epoch 122/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6423 - accuracy: 0.6077 - val_loss: 0.6711 - val_accuracy: 0.5600\n",
            "Epoch 123/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6373 - accuracy: 0.6164 - val_loss: 0.6703 - val_accuracy: 0.5700\n",
            "Epoch 124/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6598 - accuracy: 0.6070 - val_loss: 0.6693 - val_accuracy: 0.5625\n",
            "Epoch 125/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6491 - accuracy: 0.5812 - val_loss: 0.6705 - val_accuracy: 0.5600\n",
            "Epoch 126/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6418 - accuracy: 0.6274 - val_loss: 0.6692 - val_accuracy: 0.5638\n",
            "Epoch 127/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6403 - accuracy: 0.6052 - val_loss: 0.6715 - val_accuracy: 0.5675\n",
            "Epoch 128/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6403 - accuracy: 0.6120 - val_loss: 0.6696 - val_accuracy: 0.5625\n",
            "Epoch 129/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6430 - accuracy: 0.6022 - val_loss: 0.6740 - val_accuracy: 0.5725\n",
            "Epoch 130/150\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.6523 - accuracy: 0.5999 - val_loss: 0.6711 - val_accuracy: 0.5675\n",
            "Epoch 131/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6422 - accuracy: 0.6223 - val_loss: 0.6688 - val_accuracy: 0.5612\n",
            "Epoch 132/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6376 - accuracy: 0.6252 - val_loss: 0.6691 - val_accuracy: 0.5625\n",
            "Epoch 133/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6356 - accuracy: 0.6193 - val_loss: 0.6708 - val_accuracy: 0.5850\n",
            "Epoch 134/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6510 - accuracy: 0.6047 - val_loss: 0.6757 - val_accuracy: 0.5763\n",
            "Epoch 135/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6442 - accuracy: 0.6151 - val_loss: 0.6684 - val_accuracy: 0.5650\n",
            "Epoch 136/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6343 - accuracy: 0.6154 - val_loss: 0.6686 - val_accuracy: 0.5625\n",
            "Epoch 137/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6365 - accuracy: 0.6244 - val_loss: 0.6683 - val_accuracy: 0.5587\n",
            "Epoch 138/150\n",
            "38/38 [==============================] - 14s 371ms/step - loss: 0.6464 - accuracy: 0.6052 - val_loss: 0.6684 - val_accuracy: 0.5625\n",
            "Epoch 139/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6371 - accuracy: 0.6208 - val_loss: 0.6788 - val_accuracy: 0.5813\n",
            "Epoch 140/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6439 - accuracy: 0.6121 - val_loss: 0.6685 - val_accuracy: 0.5700\n",
            "Epoch 141/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6387 - accuracy: 0.6135 - val_loss: 0.6768 - val_accuracy: 0.5825\n",
            "Epoch 142/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6390 - accuracy: 0.6141 - val_loss: 0.6679 - val_accuracy: 0.5663\n",
            "Epoch 143/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6466 - accuracy: 0.6115 - val_loss: 0.6722 - val_accuracy: 0.5738\n",
            "Epoch 144/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6349 - accuracy: 0.6158 - val_loss: 0.6681 - val_accuracy: 0.5688\n",
            "Epoch 145/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6437 - accuracy: 0.5901 - val_loss: 0.6683 - val_accuracy: 0.5775\n",
            "Epoch 146/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6334 - accuracy: 0.6228 - val_loss: 0.6722 - val_accuracy: 0.5838\n",
            "Epoch 147/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6342 - accuracy: 0.6214 - val_loss: 0.6675 - val_accuracy: 0.5688\n",
            "Epoch 148/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6392 - accuracy: 0.6157 - val_loss: 0.6695 - val_accuracy: 0.5825\n",
            "Epoch 149/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6321 - accuracy: 0.6412 - val_loss: 0.6702 - val_accuracy: 0.5688\n",
            "Epoch 150/150\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.6383 - accuracy: 0.6210 - val_loss: 0.6715 - val_accuracy: 0.5738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30trUcyMTXaH"
      },
      "source": [
        "import pandas as pd\n",
        "Model_History = pd.DataFrame(history.history) \n",
        "\n",
        "hist_csv_file = '/content/drive/MyDrive/####Online_Journal/history_VGG16.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    Model_History.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1de9qUSYWIWa"
      },
      "source": [
        "############################################## VGG19 #######################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvpllAyooFoB"
      },
      "source": [
        "# ***VGG19***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rpBbKdoWaHA"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBXHbAFMXtw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b869b3f-e8c1-41c0-cfc4-49327c33c6b6"
      },
      "source": [
        "num_classes = 2\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "model = VGG19(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "#model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "  #print(layer ,layer.trainable)\n",
        "\n",
        "out = Dense(2, activation='softmax', name='output')(model.get_layer('fc2').output)\n",
        "vgg_model19 = Model(image_input, out)\n",
        "\n",
        "vgg_model19.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "574717952/574710816 [==============================] - 6s 0us/step\n",
            "574726144/574710816 [==============================] - 6s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 139,578,434\n",
            "Trainable params: 8,194\n",
            "Non-trainable params: 139,570,240\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8_j__MRZOU1"
      },
      "source": [
        "vgg_model19.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n",
        "history = vgg_model19.fit(x_train, y_train, epochs=150, validation_data=(x_test, y_test), batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjWYe2nTdF5W"
      },
      "source": [
        "import pandas as pd\n",
        "Model_History = pd.DataFrame(history.history) \n",
        "\n",
        "hist_csv_file = '/content/drive/MyDrive/####Online_Journal/history_VGG19.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    Model_History.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd2e4WhUoKAS"
      },
      "source": [
        "# ***InceptionResNetV2***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ig02IHHoSbE"
      },
      "source": [
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsNDa5xmoXB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5f993f-8348-4b06-e909-2c7cabd1344a"
      },
      "source": [
        "num_classes = 2\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "model = InceptionResNetV2(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "#model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "  #print(layer ,layer.trainable)\n",
        "\n",
        "out = Dense(2, activation='softmax', name='output')(model.get_layer('avg_pool').output)\n",
        "InceptionResNetV2_1 = Model(image_input, out)\n",
        "\n",
        "InceptionResNetV2_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
            "225214464/225209952 [==============================] - 1s 0us/step\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 25, 25, 96)   18432       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 25, 25, 64)   12288       average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 25, 25, 96)   288         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 25, 25, 64)   192         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 25, 25, 96)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 25, 25, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed_5b (Concatenate)          (None, 25, 25, 320)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 25, 25, 32)   96          conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 25, 25, 32)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 25, 25, 48)   13824       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 25, 25, 32)   96          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 25, 25, 48)   144         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 25, 25, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 25, 25, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 25, 25, 32)   9216        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 25, 25, 64)   27648       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 25, 25, 32)   96          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 25, 25, 32)   96          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 25, 25, 64)   192         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 25, 25, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 25, 25, 32)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 25, 25, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_1_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_1 (Lambda)              (None, 25, 25, 320)  0           mixed_5b[0][0]                   \n",
            "                                                                 block35_1_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_ac (Activation)       (None, 25, 25, 320)  0           block35_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 25, 25, 32)   96          conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 25, 25, 32)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 25, 25, 48)   13824       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 25, 25, 32)   96          conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 25, 25, 48)   144         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 25, 25, 32)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 25, 25, 48)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 25, 25, 32)   9216        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 25, 25, 64)   27648       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 25, 25, 32)   96          conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 25, 25, 32)   96          conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 25, 25, 64)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 25, 25, 32)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 25, 25, 32)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 25, 25, 64)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_18[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_2_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_2 (Lambda)              (None, 25, 25, 320)  0           block35_1_ac[0][0]               \n",
            "                                                                 block35_2_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_ac (Activation)       (None, 25, 25, 320)  0           block35_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 25, 25, 32)   96          conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 25, 25, 32)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 25, 25, 48)   13824       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 25, 25, 32)   96          conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 25, 25, 48)   144         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 25, 25, 32)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 25, 25, 48)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 25, 25, 32)   9216        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 25, 25, 64)   27648       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 25, 25, 32)   96          conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 25, 25, 32)   96          conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 25, 25, 64)   192         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 25, 25, 32)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 25, 25, 32)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 25, 25, 64)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_24[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_3_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_3 (Lambda)              (None, 25, 25, 320)  0           block35_2_ac[0][0]               \n",
            "                                                                 block35_3_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_ac (Activation)       (None, 25, 25, 320)  0           block35_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 25, 25, 32)   96          conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 25, 25, 32)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 25, 25, 48)   13824       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 25, 25, 32)   96          conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 25, 25, 48)   144         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 25, 25, 32)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 25, 25, 48)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 25, 25, 32)   9216        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 25, 25, 64)   27648       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 25, 25, 32)   96          conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 25, 25, 32)   96          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 25, 25, 64)   192         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 25, 25, 32)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 25, 25, 32)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 25, 25, 64)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_30[0][0]              \n",
            "                                                                 activation_32[0][0]              \n",
            "                                                                 activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_4_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_4 (Lambda)              (None, 25, 25, 320)  0           block35_3_ac[0][0]               \n",
            "                                                                 block35_4_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_ac (Activation)       (None, 25, 25, 320)  0           block35_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 25, 25, 32)   96          conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 25, 25, 32)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 25, 25, 48)   13824       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 25, 25, 32)   96          conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 25, 25, 48)   144         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 25, 25, 32)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 25, 25, 48)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 25, 25, 32)   9216        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 25, 25, 64)   27648       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 25, 25, 32)   96          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 25, 25, 32)   96          conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 25, 25, 64)   192         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 25, 25, 32)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 25, 25, 32)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 25, 25, 64)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_36[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_5_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_5 (Lambda)              (None, 25, 25, 320)  0           block35_4_ac[0][0]               \n",
            "                                                                 block35_5_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_ac (Activation)       (None, 25, 25, 320)  0           block35_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 25, 25, 32)   96          conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 25, 25, 32)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 25, 25, 48)   13824       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 25, 25, 32)   96          conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 25, 25, 48)   144         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 25, 25, 32)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 25, 25, 48)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 25, 25, 32)   9216        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 25, 25, 64)   27648       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 25, 25, 32)   96          conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 25, 25, 32)   96          conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 25, 25, 64)   192         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 25, 25, 32)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 25, 25, 32)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 25, 25, 64)   0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_42[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "                                                                 activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_6_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_6 (Lambda)              (None, 25, 25, 320)  0           block35_5_ac[0][0]               \n",
            "                                                                 block35_6_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_ac (Activation)       (None, 25, 25, 320)  0           block35_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 25, 25, 32)   96          conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 25, 25, 32)   0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 25, 25, 48)   13824       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 25, 25, 32)   96          conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 25, 25, 48)   144         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 25, 25, 32)   0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 25, 25, 48)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 25, 25, 32)   9216        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 25, 25, 64)   27648       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 25, 25, 32)   96          conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 25, 25, 32)   96          conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 25, 25, 64)   192         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 25, 25, 32)   0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 25, 25, 32)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 25, 25, 64)   0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_48[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_7_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_7 (Lambda)              (None, 25, 25, 320)  0           block35_6_ac[0][0]               \n",
            "                                                                 block35_7_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_ac (Activation)       (None, 25, 25, 320)  0           block35_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 25, 25, 32)   96          conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 25, 25, 32)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 25, 25, 48)   13824       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 25, 25, 32)   96          conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 25, 25, 48)   144         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 25, 25, 32)   0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 25, 25, 48)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 25, 25, 32)   9216        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 25, 25, 64)   27648       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 25, 25, 32)   96          conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 25, 25, 32)   96          conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 25, 25, 64)   192         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 25, 25, 32)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 25, 25, 32)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 25, 25, 64)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_54[0][0]              \n",
            "                                                                 activation_56[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_8_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_8 (Lambda)              (None, 25, 25, 320)  0           block35_7_ac[0][0]               \n",
            "                                                                 block35_8_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_ac (Activation)       (None, 25, 25, 320)  0           block35_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 25, 25, 32)   96          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 25, 25, 32)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 25, 25, 48)   13824       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 25, 25, 32)   96          conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 25, 25, 48)   144         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 25, 25, 32)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 25, 25, 48)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 25, 25, 32)   9216        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 25, 25, 64)   27648       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 25, 25, 32)   96          conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 25, 25, 32)   96          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 25, 25, 64)   192         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 25, 25, 32)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 25, 25, 32)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 25, 25, 64)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_60[0][0]              \n",
            "                                                                 activation_62[0][0]              \n",
            "                                                                 activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_9_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_9 (Lambda)              (None, 25, 25, 320)  0           block35_8_ac[0][0]               \n",
            "                                                                 block35_9_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_ac (Activation)       (None, 25, 25, 320)  0           block35_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 25, 25, 32)   96          conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 25, 25, 32)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 25, 25, 48)   13824       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 25, 25, 32)   96          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 25, 25, 48)   144         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 25, 25, 32)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 25, 25, 48)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 25, 25, 32)   9216        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 25, 25, 64)   27648       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 25, 25, 32)   96          conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 25, 25, 32)   96          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 25, 25, 64)   192         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 25, 25, 32)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 25, 25, 32)   0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 25, 25, 64)   0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_mixed (Concatenate)  (None, 25, 25, 128)  0           activation_66[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_conv (Conv2D)        (None, 25, 25, 320)  41280       block35_10_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block35_10 (Lambda)             (None, 25, 25, 320)  0           block35_9_ac[0][0]               \n",
            "                                                                 block35_10_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_ac (Activation)      (None, 25, 25, 320)  0           block35_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 25, 25, 256)  81920       block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 25, 25, 256)  768         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 25, 25, 256)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 25, 25, 256)  589824      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 25, 25, 256)  768         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 25, 25, 256)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 12, 12, 384)  1105920     block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 12, 12, 384)  884736      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 12, 12, 384)  1152        conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 12, 12, 384)  1152        conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 12, 12, 384)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 12, 12, 384)  0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 320)  0           block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mixed_6a (Concatenate)          (None, 12, 12, 1088) 0           activation_72[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 12, 12, 128)  139264      mixed_6a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 12, 12, 128)  384         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 12, 12, 128)  0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 12, 12, 160)  143360      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 12, 12, 160)  480         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 12, 12, 160)  0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 12, 12, 192)  208896      mixed_6a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 12, 12, 192)  215040      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 12, 12, 192)  576         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 12, 12, 192)  576         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 12, 12, 192)  0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 12, 12, 192)  0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_76[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_1_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_1 (Lambda)              (None, 12, 12, 1088) 0           mixed_6a[0][0]                   \n",
            "                                                                 block17_1_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_ac (Activation)       (None, 12, 12, 1088) 0           block17_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 12, 12, 128)  139264      block17_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 12, 12, 128)  384         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 12, 12, 128)  0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 12, 12, 160)  143360      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 12, 12, 160)  480         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 12, 12, 160)  0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 12, 12, 192)  208896      block17_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 12, 12, 192)  215040      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 12, 12, 192)  576         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 12, 12, 192)  576         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 12, 12, 192)  0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 12, 12, 192)  0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_80[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_2_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_2 (Lambda)              (None, 12, 12, 1088) 0           block17_1_ac[0][0]               \n",
            "                                                                 block17_2_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_ac (Activation)       (None, 12, 12, 1088) 0           block17_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 12, 12, 128)  139264      block17_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 12, 12, 128)  384         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 12, 12, 128)  0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 12, 12, 160)  143360      activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 12, 12, 160)  480         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 12, 12, 160)  0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 12, 12, 192)  208896      block17_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 12, 12, 192)  215040      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 12, 12, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 12, 12, 192)  576         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 12, 12, 192)  0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 12, 12, 192)  0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_84[0][0]              \n",
            "                                                                 activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_3_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_3 (Lambda)              (None, 12, 12, 1088) 0           block17_2_ac[0][0]               \n",
            "                                                                 block17_3_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_ac (Activation)       (None, 12, 12, 1088) 0           block17_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 12, 12, 128)  139264      block17_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 12, 12, 128)  384         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 12, 12, 128)  0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 12, 12, 160)  143360      activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 12, 12, 160)  480         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 12, 12, 160)  0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 12, 12, 192)  208896      block17_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 12, 12, 192)  215040      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 12, 12, 192)  576         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 12, 12, 192)  576         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 12, 12, 192)  0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 12, 12, 192)  0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_88[0][0]              \n",
            "                                                                 activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_4_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_4 (Lambda)              (None, 12, 12, 1088) 0           block17_3_ac[0][0]               \n",
            "                                                                 block17_4_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_ac (Activation)       (None, 12, 12, 1088) 0           block17_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 12, 12, 128)  139264      block17_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 12, 12, 128)  384         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 12, 12, 128)  0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 12, 12, 160)  143360      activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 12, 12, 160)  480         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 12, 12, 160)  0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 12, 12, 192)  208896      block17_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 12, 12, 192)  215040      activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 12, 12, 192)  576         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 12, 12, 192)  576         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 12, 12, 192)  0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 12, 12, 192)  0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_92[0][0]              \n",
            "                                                                 activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_5_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_5 (Lambda)              (None, 12, 12, 1088) 0           block17_4_ac[0][0]               \n",
            "                                                                 block17_5_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_ac (Activation)       (None, 12, 12, 1088) 0           block17_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 12, 12, 128)  139264      block17_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 12, 12, 128)  384         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 12, 12, 128)  0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 12, 12, 160)  143360      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 12, 12, 160)  480         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 12, 12, 160)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 12, 12, 192)  208896      block17_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 12, 12, 192)  215040      activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 12, 12, 192)  576         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 12, 12, 192)  576         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 12, 12, 192)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 12, 12, 192)  0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_96[0][0]              \n",
            "                                                                 activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_6_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_6 (Lambda)              (None, 12, 12, 1088) 0           block17_5_ac[0][0]               \n",
            "                                                                 block17_6_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_ac (Activation)       (None, 12, 12, 1088) 0           block17_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 12, 12, 128)  139264      block17_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 12, 12, 128)  384         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 12, 12, 128)  0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 12, 12, 160)  143360      activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 12, 12, 160)  480         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 12, 12, 160)  0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 12, 12, 192)  208896      block17_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 12, 12, 192)  215040      activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 12, 12, 192)  576         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 12, 12, 192)  576         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 12, 12, 192)  0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 12, 12, 192)  0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_100[0][0]             \n",
            "                                                                 activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_7_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_7 (Lambda)              (None, 12, 12, 1088) 0           block17_6_ac[0][0]               \n",
            "                                                                 block17_7_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_ac (Activation)       (None, 12, 12, 1088) 0           block17_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 12, 12, 128)  139264      block17_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 12, 12, 128)  384         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 12, 12, 128)  0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 12, 12, 160)  143360      activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 12, 12, 160)  480         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 12, 12, 160)  0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 12, 12, 192)  208896      block17_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 12, 12, 192)  215040      activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 12, 12, 192)  576         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 12, 12, 192)  576         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 12, 12, 192)  0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 12, 12, 192)  0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_104[0][0]             \n",
            "                                                                 activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_8_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_8 (Lambda)              (None, 12, 12, 1088) 0           block17_7_ac[0][0]               \n",
            "                                                                 block17_8_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_ac (Activation)       (None, 12, 12, 1088) 0           block17_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 12, 12, 128)  139264      block17_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 12, 12, 128)  384         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 12, 12, 128)  0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 12, 12, 160)  143360      activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 12, 12, 160)  480         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 12, 12, 160)  0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 12, 12, 192)  208896      block17_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 12, 12, 192)  215040      activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 12, 12, 192)  576         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 12, 12, 192)  576         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 12, 12, 192)  0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 12, 12, 192)  0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_9_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_9 (Lambda)              (None, 12, 12, 1088) 0           block17_8_ac[0][0]               \n",
            "                                                                 block17_9_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_ac (Activation)       (None, 12, 12, 1088) 0           block17_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 12, 12, 128)  139264      block17_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 12, 12, 128)  384         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 12, 12, 128)  0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 12, 12, 160)  143360      activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 12, 12, 160)  480         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 12, 12, 160)  0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 12, 12, 192)  208896      block17_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 12, 12, 192)  215040      activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 12, 12, 192)  576         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 12, 12, 192)  576         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 12, 12, 192)  0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 12, 12, 192)  0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_112[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_10_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_10 (Lambda)             (None, 12, 12, 1088) 0           block17_9_ac[0][0]               \n",
            "                                                                 block17_10_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_ac (Activation)      (None, 12, 12, 1088) 0           block17_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 12, 12, 128)  139264      block17_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 12, 12, 128)  384         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 12, 12, 128)  0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 12, 12, 160)  143360      activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 12, 12, 160)  480         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 12, 12, 160)  0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 12, 12, 192)  208896      block17_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 12, 12, 192)  215040      activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 12, 12, 192)  576         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 12, 12, 192)  576         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 12, 12, 192)  0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 12, 12, 192)  0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_116[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_11_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_11 (Lambda)             (None, 12, 12, 1088) 0           block17_10_ac[0][0]              \n",
            "                                                                 block17_11_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_ac (Activation)      (None, 12, 12, 1088) 0           block17_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 12, 12, 128)  139264      block17_11_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 12, 12, 128)  384         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 12, 12, 128)  0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 12, 12, 160)  143360      activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 12, 12, 160)  480         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 12, 12, 160)  0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 12, 12, 192)  208896      block17_11_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 12, 12, 192)  215040      activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 12, 12, 192)  576         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 12, 12, 192)  576         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 12, 12, 192)  0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 12, 12, 192)  0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_12_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_12 (Lambda)             (None, 12, 12, 1088) 0           block17_11_ac[0][0]              \n",
            "                                                                 block17_12_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_ac (Activation)      (None, 12, 12, 1088) 0           block17_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 12, 12, 128)  139264      block17_12_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 12, 12, 128)  384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 12, 12, 128)  0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 12, 12, 160)  143360      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 12, 12, 160)  480         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 12, 12, 160)  0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 12, 12, 192)  208896      block17_12_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 12, 12, 192)  215040      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 12, 12, 192)  576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 12, 12, 192)  576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 12, 12, 192)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 12, 12, 192)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_13_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_13 (Lambda)             (None, 12, 12, 1088) 0           block17_12_ac[0][0]              \n",
            "                                                                 block17_13_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_ac (Activation)      (None, 12, 12, 1088) 0           block17_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 12, 12, 128)  139264      block17_13_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 12, 12, 128)  384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 12, 12, 128)  0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 12, 12, 160)  143360      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 12, 12, 160)  480         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 12, 12, 160)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 12, 12, 192)  208896      block17_13_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 12, 12, 192)  215040      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 12, 12, 192)  576         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 12, 12, 192)  576         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 12, 12, 192)  0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 12, 12, 192)  0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_128[0][0]             \n",
            "                                                                 activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_14_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_14 (Lambda)             (None, 12, 12, 1088) 0           block17_13_ac[0][0]              \n",
            "                                                                 block17_14_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_ac (Activation)      (None, 12, 12, 1088) 0           block17_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 12, 12, 128)  139264      block17_14_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 12, 12, 128)  384         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 12, 12, 128)  0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 12, 12, 160)  143360      activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 12, 12, 160)  480         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 12, 12, 160)  0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 12, 12, 192)  208896      block17_14_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 12, 12, 192)  215040      activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 12, 12, 192)  576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 12, 12, 192)  576         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 12, 12, 192)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 12, 12, 192)  0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_132[0][0]             \n",
            "                                                                 activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_15_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_15 (Lambda)             (None, 12, 12, 1088) 0           block17_14_ac[0][0]              \n",
            "                                                                 block17_15_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_ac (Activation)      (None, 12, 12, 1088) 0           block17_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 12, 12, 128)  139264      block17_15_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 12, 12, 128)  384         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 12, 12, 128)  0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 12, 12, 160)  143360      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 12, 12, 160)  480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 12, 12, 160)  0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 12, 12, 192)  208896      block17_15_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 12, 12, 192)  215040      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 12, 12, 192)  576         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 12, 12, 192)  576         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 12, 12, 192)  0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 12, 12, 192)  0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_136[0][0]             \n",
            "                                                                 activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_16_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_16 (Lambda)             (None, 12, 12, 1088) 0           block17_15_ac[0][0]              \n",
            "                                                                 block17_16_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_ac (Activation)      (None, 12, 12, 1088) 0           block17_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 12, 12, 128)  139264      block17_16_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 12, 12, 128)  384         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 12, 12, 128)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 12, 12, 160)  143360      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 12, 12, 160)  480         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 12, 12, 160)  0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 12, 12, 192)  208896      block17_16_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 12, 12, 192)  215040      activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 12, 12, 192)  576         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 12, 12, 192)  576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 12, 12, 192)  0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 12, 12, 192)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_140[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_17_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_17 (Lambda)             (None, 12, 12, 1088) 0           block17_16_ac[0][0]              \n",
            "                                                                 block17_17_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_ac (Activation)      (None, 12, 12, 1088) 0           block17_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 12, 12, 128)  139264      block17_17_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 12, 12, 128)  384         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 12, 12, 128)  0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 12, 12, 160)  143360      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 12, 12, 160)  480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 12, 12, 160)  0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 12, 12, 192)  208896      block17_17_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 12, 12, 192)  215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 12, 12, 192)  576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 12, 12, 192)  576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 12, 12, 192)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 12, 12, 192)  0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_18_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_18 (Lambda)             (None, 12, 12, 1088) 0           block17_17_ac[0][0]              \n",
            "                                                                 block17_18_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_ac (Activation)      (None, 12, 12, 1088) 0           block17_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 12, 12, 128)  139264      block17_18_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 12, 12, 128)  384         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 12, 12, 128)  0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 12, 12, 160)  143360      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 12, 12, 160)  480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 12, 12, 160)  0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 12, 12, 192)  208896      block17_18_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 12, 12, 192)  215040      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 12, 12, 192)  576         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 12, 12, 192)  576         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 12, 12, 192)  0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 12, 12, 192)  0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_148[0][0]             \n",
            "                                                                 activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_19_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_19 (Lambda)             (None, 12, 12, 1088) 0           block17_18_ac[0][0]              \n",
            "                                                                 block17_19_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_ac (Activation)      (None, 12, 12, 1088) 0           block17_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 12, 12, 128)  139264      block17_19_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 12, 12, 128)  384         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 12, 12, 128)  0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 12, 12, 160)  143360      activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 12, 12, 160)  480         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 12, 12, 160)  0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 12, 12, 192)  208896      block17_19_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 12, 12, 192)  215040      activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 12, 12, 192)  576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 12, 12, 192)  576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 12, 12, 192)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 12, 12, 192)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_152[0][0]             \n",
            "                                                                 activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_20_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_20 (Lambda)             (None, 12, 12, 1088) 0           block17_19_ac[0][0]              \n",
            "                                                                 block17_20_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_ac (Activation)      (None, 12, 12, 1088) 0           block17_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 12, 12, 256)  768         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 12, 12, 256)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 12, 12, 288)  663552      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 12, 12, 256)  768         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 12, 12, 256)  768         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 12, 12, 288)  864         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 12, 12, 256)  0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 12, 12, 256)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 12, 12, 288)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 5, 5, 384)    884736      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 5, 5, 288)    663552      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 5, 5, 320)    829440      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 5, 5, 384)    1152        conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 5, 5, 288)    864         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 5, 5, 320)    960         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 5, 5, 384)    0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 5, 5, 288)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 5, 5, 320)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 1088)   0           block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mixed_7a (Concatenate)          (None, 5, 5, 2080)   0           activation_157[0][0]             \n",
            "                                                                 activation_159[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 5, 5, 192)    399360      mixed_7a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 5, 5, 192)    576         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 5, 5, 192)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 5, 5, 224)    129024      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 5, 5, 224)    672         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 5, 5, 224)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 5, 5, 192)    399360      mixed_7a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 5, 5, 256)    172032      activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 5, 5, 192)    576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 5, 5, 256)    768         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 5, 5, 192)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 5, 5, 256)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_163[0][0]             \n",
            "                                                                 activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_1_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_1 (Lambda)               (None, 5, 5, 2080)   0           mixed_7a[0][0]                   \n",
            "                                                                 block8_1_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_ac (Activation)        (None, 5, 5, 2080)   0           block8_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 5, 5, 192)    399360      block8_1_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 5, 5, 192)    576         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 5, 5, 192)    0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 5, 5, 224)    129024      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 5, 5, 224)    672         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 5, 5, 224)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 5, 5, 192)    399360      block8_1_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 5, 5, 256)    172032      activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 5, 5, 192)    576         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 5, 5, 256)    768         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 5, 5, 192)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 5, 5, 256)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_167[0][0]             \n",
            "                                                                 activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_2_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_2 (Lambda)               (None, 5, 5, 2080)   0           block8_1_ac[0][0]                \n",
            "                                                                 block8_2_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_ac (Activation)        (None, 5, 5, 2080)   0           block8_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 5, 5, 192)    399360      block8_2_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 5, 5, 192)    576         conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 5, 5, 192)    0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 5, 5, 224)    129024      activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 5, 5, 224)    672         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 5, 5, 224)    0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 5, 5, 192)    399360      block8_2_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 5, 5, 256)    172032      activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 5, 5, 192)    576         conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 5, 5, 256)    768         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 5, 5, 192)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 5, 5, 256)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_171[0][0]             \n",
            "                                                                 activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_3_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_3 (Lambda)               (None, 5, 5, 2080)   0           block8_2_ac[0][0]                \n",
            "                                                                 block8_3_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_ac (Activation)        (None, 5, 5, 2080)   0           block8_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 5, 5, 192)    399360      block8_3_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 5, 5, 192)    576         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 5, 5, 192)    0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 5, 5, 224)    129024      activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 5, 5, 224)    672         conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 5, 5, 224)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 5, 5, 192)    399360      block8_3_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 5, 5, 256)    172032      activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 5, 5, 192)    576         conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 5, 5, 256)    768         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 5, 5, 192)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 5, 5, 256)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_175[0][0]             \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_4_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_4 (Lambda)               (None, 5, 5, 2080)   0           block8_3_ac[0][0]                \n",
            "                                                                 block8_4_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_ac (Activation)        (None, 5, 5, 2080)   0           block8_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 5, 5, 192)    399360      block8_4_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 5, 5, 192)    576         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 5, 5, 192)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 5, 5, 224)    129024      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 5, 5, 224)    672         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 5, 5, 224)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 5, 5, 192)    399360      block8_4_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 5, 5, 256)    172032      activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 5, 5, 192)    576         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 5, 5, 256)    768         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 5, 5, 192)    0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 5, 5, 256)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_179[0][0]             \n",
            "                                                                 activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_5_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_5 (Lambda)               (None, 5, 5, 2080)   0           block8_4_ac[0][0]                \n",
            "                                                                 block8_5_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_ac (Activation)        (None, 5, 5, 2080)   0           block8_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 5, 5, 192)    399360      block8_5_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 5, 5, 192)    576         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 5, 5, 192)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 5, 5, 224)    129024      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 5, 5, 224)    672         conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 5, 5, 224)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 5, 5, 192)    399360      block8_5_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 5, 5, 256)    172032      activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 5, 5, 192)    576         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 5, 5, 256)    768         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 5, 5, 192)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 5, 5, 256)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_183[0][0]             \n",
            "                                                                 activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_6_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_6 (Lambda)               (None, 5, 5, 2080)   0           block8_5_ac[0][0]                \n",
            "                                                                 block8_6_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_ac (Activation)        (None, 5, 5, 2080)   0           block8_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 5, 5, 192)    399360      block8_6_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 5, 5, 192)    576         conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 5, 5, 192)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 5, 5, 224)    129024      activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 5, 5, 224)    672         conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 5, 5, 224)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 5, 5, 192)    399360      block8_6_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 5, 5, 256)    172032      activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 5, 5, 192)    576         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 5, 5, 256)    768         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 5, 5, 192)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 5, 5, 256)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_187[0][0]             \n",
            "                                                                 activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_7_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_7 (Lambda)               (None, 5, 5, 2080)   0           block8_6_ac[0][0]                \n",
            "                                                                 block8_7_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_ac (Activation)        (None, 5, 5, 2080)   0           block8_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 5, 5, 192)    399360      block8_7_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 5, 5, 192)    576         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 5, 5, 192)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 5, 5, 224)    129024      activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 5, 5, 224)    672         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 5, 5, 224)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 5, 5, 192)    399360      block8_7_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 5, 5, 256)    172032      activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 5, 5, 192)    576         conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 5, 5, 256)    768         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 5, 5, 192)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 5, 5, 256)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_191[0][0]             \n",
            "                                                                 activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_8_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_8 (Lambda)               (None, 5, 5, 2080)   0           block8_7_ac[0][0]                \n",
            "                                                                 block8_8_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_ac (Activation)        (None, 5, 5, 2080)   0           block8_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 5, 5, 192)    399360      block8_8_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 5, 5, 192)    576         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 5, 5, 192)    0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 5, 5, 224)    129024      activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 5, 5, 224)    672         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 5, 5, 224)    0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 5, 5, 192)    399360      block8_8_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 5, 5, 256)    172032      activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 5, 5, 192)    576         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 5, 5, 256)    768         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 5, 5, 192)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 5, 5, 256)    0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_195[0][0]             \n",
            "                                                                 activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_9_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_9 (Lambda)               (None, 5, 5, 2080)   0           block8_8_ac[0][0]                \n",
            "                                                                 block8_9_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_ac (Activation)        (None, 5, 5, 2080)   0           block8_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 5, 5, 192)    399360      block8_9_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 5, 5, 192)    576         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 5, 5, 192)    0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 5, 5, 224)    129024      activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 5, 5, 224)    672         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 5, 5, 224)    0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 5, 5, 192)    399360      block8_9_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 5, 5, 256)    172032      activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 5, 5, 192)    576         conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 5, 5, 256)    768         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 5, 5, 192)    0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 5, 5, 256)    0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_10_mixed (Concatenate)   (None, 5, 5, 448)    0           activation_199[0][0]             \n",
            "                                                                 activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_10_conv (Conv2D)         (None, 5, 5, 2080)   933920      block8_10_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_10 (Lambda)              (None, 5, 5, 2080)   0           block8_9_ac[0][0]                \n",
            "                                                                 block8_10_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b (Conv2D)                (None, 5, 5, 1536)   3194880     block8_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b_bn (BatchNormalization) (None, 5, 5, 1536)   4608        conv_7b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b_ac (Activation)         (None, 5, 5, 1536)   0           conv_7b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 1536)         0           conv_7b_ac[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 2)            3074        avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 54,339,810\n",
            "Trainable params: 3,074\n",
            "Non-trainable params: 54,336,736\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYV5e0BipV35",
        "outputId": "5c6c4b98-e945-40a0-812d-1d082cf10d33"
      },
      "source": [
        "InceptionResNetV2_1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n",
        "history = InceptionResNetV2_1.fit(x_train, y_train, epochs=150, validation_data=(x_test, y_test), batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "38/38 [==============================] - 30s 513ms/step - loss: 0.6930 - accuracy: 0.5151 - val_loss: 0.6931 - val_accuracy: 0.5275\n",
            "Epoch 2/150\n",
            "38/38 [==============================] - 15s 406ms/step - loss: 0.6879 - accuracy: 0.5407 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
            "Epoch 3/150\n",
            "38/38 [==============================] - 16s 411ms/step - loss: 0.6885 - accuracy: 0.5309 - val_loss: 0.6990 - val_accuracy: 0.5088\n",
            "Epoch 4/150\n",
            "38/38 [==============================] - 16s 420ms/step - loss: 0.6881 - accuracy: 0.5402 - val_loss: 0.6925 - val_accuracy: 0.5175\n",
            "Epoch 5/150\n",
            "38/38 [==============================] - 16s 432ms/step - loss: 0.6812 - accuracy: 0.5523 - val_loss: 0.6932 - val_accuracy: 0.5213\n",
            "Epoch 6/150\n",
            "38/38 [==============================] - 16s 437ms/step - loss: 0.6817 - accuracy: 0.5601 - val_loss: 0.6894 - val_accuracy: 0.5312\n",
            "Epoch 7/150\n",
            "38/38 [==============================] - 17s 441ms/step - loss: 0.6789 - accuracy: 0.5517 - val_loss: 0.6888 - val_accuracy: 0.5412\n",
            "Epoch 8/150\n",
            "38/38 [==============================] - 17s 449ms/step - loss: 0.6790 - accuracy: 0.5642 - val_loss: 0.6882 - val_accuracy: 0.5263\n",
            "Epoch 9/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6721 - accuracy: 0.5677 - val_loss: 0.7019 - val_accuracy: 0.5375\n",
            "Epoch 10/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6803 - accuracy: 0.5531 - val_loss: 0.6880 - val_accuracy: 0.5288\n",
            "Epoch 11/150\n",
            "38/38 [==============================] - 17s 449ms/step - loss: 0.6751 - accuracy: 0.5485 - val_loss: 0.6880 - val_accuracy: 0.5275\n",
            "Epoch 12/150\n",
            "38/38 [==============================] - 17s 447ms/step - loss: 0.6749 - accuracy: 0.5700 - val_loss: 0.6891 - val_accuracy: 0.5063\n",
            "Epoch 13/150\n",
            "38/38 [==============================] - 17s 448ms/step - loss: 0.6760 - accuracy: 0.5394 - val_loss: 0.6875 - val_accuracy: 0.5437\n",
            "Epoch 14/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6736 - accuracy: 0.5621 - val_loss: 0.6871 - val_accuracy: 0.5437\n",
            "Epoch 15/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6684 - accuracy: 0.5742 - val_loss: 0.6999 - val_accuracy: 0.5462\n",
            "Epoch 16/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6770 - accuracy: 0.5676 - val_loss: 0.6883 - val_accuracy: 0.5225\n",
            "Epoch 17/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6681 - accuracy: 0.5785 - val_loss: 0.6870 - val_accuracy: 0.5263\n",
            "Epoch 18/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6629 - accuracy: 0.6053 - val_loss: 0.6899 - val_accuracy: 0.5375\n",
            "Epoch 19/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6644 - accuracy: 0.5588 - val_loss: 0.6994 - val_accuracy: 0.5500\n",
            "Epoch 20/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6661 - accuracy: 0.5856 - val_loss: 0.6965 - val_accuracy: 0.5450\n",
            "Epoch 21/150\n",
            "38/38 [==============================] - 17s 449ms/step - loss: 0.6651 - accuracy: 0.5841 - val_loss: 0.7008 - val_accuracy: 0.5462\n",
            "Epoch 22/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6721 - accuracy: 0.5648 - val_loss: 0.6878 - val_accuracy: 0.5300\n",
            "Epoch 23/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6647 - accuracy: 0.5943 - val_loss: 0.6873 - val_accuracy: 0.5288\n",
            "Epoch 24/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6669 - accuracy: 0.5773 - val_loss: 0.6871 - val_accuracy: 0.5312\n",
            "Epoch 25/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6615 - accuracy: 0.5978 - val_loss: 0.6951 - val_accuracy: 0.5412\n",
            "Epoch 26/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6604 - accuracy: 0.5911 - val_loss: 0.6946 - val_accuracy: 0.5412\n",
            "Epoch 27/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6611 - accuracy: 0.5923 - val_loss: 0.6892 - val_accuracy: 0.5375\n",
            "Epoch 28/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6635 - accuracy: 0.5884 - val_loss: 0.6880 - val_accuracy: 0.5275\n",
            "Epoch 29/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6650 - accuracy: 0.5811 - val_loss: 0.6867 - val_accuracy: 0.5288\n",
            "Epoch 30/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6638 - accuracy: 0.5879 - val_loss: 0.6866 - val_accuracy: 0.5275\n",
            "Epoch 31/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6616 - accuracy: 0.5750 - val_loss: 0.6908 - val_accuracy: 0.5425\n",
            "Epoch 32/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6605 - accuracy: 0.5782 - val_loss: 0.6966 - val_accuracy: 0.5400\n",
            "Epoch 33/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6624 - accuracy: 0.5967 - val_loss: 0.6868 - val_accuracy: 0.5312\n",
            "Epoch 34/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6541 - accuracy: 0.5989 - val_loss: 0.6890 - val_accuracy: 0.5325\n",
            "Epoch 35/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6602 - accuracy: 0.5905 - val_loss: 0.6884 - val_accuracy: 0.5350\n",
            "Epoch 36/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6585 - accuracy: 0.5962 - val_loss: 0.6875 - val_accuracy: 0.5337\n",
            "Epoch 37/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6672 - accuracy: 0.5852 - val_loss: 0.6887 - val_accuracy: 0.5325\n",
            "Epoch 38/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6624 - accuracy: 0.5873 - val_loss: 0.6888 - val_accuracy: 0.5337\n",
            "Epoch 39/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6633 - accuracy: 0.5886 - val_loss: 0.6873 - val_accuracy: 0.5250\n",
            "Epoch 40/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6553 - accuracy: 0.6126 - val_loss: 0.6903 - val_accuracy: 0.5312\n",
            "Epoch 41/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6617 - accuracy: 0.5810 - val_loss: 0.6924 - val_accuracy: 0.5500\n",
            "Epoch 42/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6577 - accuracy: 0.6071 - val_loss: 0.6904 - val_accuracy: 0.5412\n",
            "Epoch 43/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6560 - accuracy: 0.5880 - val_loss: 0.6947 - val_accuracy: 0.5462\n",
            "Epoch 44/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6529 - accuracy: 0.6023 - val_loss: 0.6878 - val_accuracy: 0.5312\n",
            "Epoch 45/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6679 - accuracy: 0.5685 - val_loss: 0.6871 - val_accuracy: 0.5275\n",
            "Epoch 46/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6586 - accuracy: 0.5904 - val_loss: 0.6883 - val_accuracy: 0.5362\n",
            "Epoch 47/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6576 - accuracy: 0.5931 - val_loss: 0.6878 - val_accuracy: 0.5362\n",
            "Epoch 48/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6587 - accuracy: 0.5893 - val_loss: 0.6896 - val_accuracy: 0.5350\n",
            "Epoch 49/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6535 - accuracy: 0.6156 - val_loss: 0.6889 - val_accuracy: 0.5288\n",
            "Epoch 50/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6642 - accuracy: 0.5768 - val_loss: 0.6868 - val_accuracy: 0.5250\n",
            "Epoch 51/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6536 - accuracy: 0.5928 - val_loss: 0.6882 - val_accuracy: 0.5387\n",
            "Epoch 52/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6505 - accuracy: 0.6205 - val_loss: 0.6882 - val_accuracy: 0.5425\n",
            "Epoch 53/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6634 - accuracy: 0.5932 - val_loss: 0.6878 - val_accuracy: 0.5375\n",
            "Epoch 54/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6510 - accuracy: 0.6221 - val_loss: 0.6869 - val_accuracy: 0.5300\n",
            "Epoch 55/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6542 - accuracy: 0.6100 - val_loss: 0.6873 - val_accuracy: 0.5312\n",
            "Epoch 56/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6513 - accuracy: 0.6096 - val_loss: 0.6892 - val_accuracy: 0.5362\n",
            "Epoch 57/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6583 - accuracy: 0.5928 - val_loss: 0.6912 - val_accuracy: 0.5375\n",
            "Epoch 58/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6464 - accuracy: 0.6182 - val_loss: 0.7013 - val_accuracy: 0.5375\n",
            "Epoch 59/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6555 - accuracy: 0.6002 - val_loss: 0.6868 - val_accuracy: 0.5300\n",
            "Epoch 60/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6519 - accuracy: 0.5917 - val_loss: 0.6977 - val_accuracy: 0.5412\n",
            "Epoch 61/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6496 - accuracy: 0.6196 - val_loss: 0.6870 - val_accuracy: 0.5300\n",
            "Epoch 62/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6514 - accuracy: 0.6079 - val_loss: 0.7051 - val_accuracy: 0.5462\n",
            "Epoch 63/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6558 - accuracy: 0.5900 - val_loss: 0.6901 - val_accuracy: 0.5412\n",
            "Epoch 64/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6569 - accuracy: 0.6001 - val_loss: 0.6868 - val_accuracy: 0.5288\n",
            "Epoch 65/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6581 - accuracy: 0.5997 - val_loss: 0.6955 - val_accuracy: 0.5425\n",
            "Epoch 66/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6485 - accuracy: 0.6211 - val_loss: 0.6871 - val_accuracy: 0.5362\n",
            "Epoch 67/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6483 - accuracy: 0.6281 - val_loss: 0.6875 - val_accuracy: 0.5375\n",
            "Epoch 68/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6457 - accuracy: 0.6246 - val_loss: 0.6884 - val_accuracy: 0.5400\n",
            "Epoch 69/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6482 - accuracy: 0.6231 - val_loss: 0.6941 - val_accuracy: 0.5387\n",
            "Epoch 70/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6511 - accuracy: 0.6012 - val_loss: 0.6869 - val_accuracy: 0.5350\n",
            "Epoch 71/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6595 - accuracy: 0.5950 - val_loss: 0.6983 - val_accuracy: 0.5387\n",
            "Epoch 72/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6619 - accuracy: 0.5882 - val_loss: 0.6886 - val_accuracy: 0.5375\n",
            "Epoch 73/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6602 - accuracy: 0.6037 - val_loss: 0.6864 - val_accuracy: 0.5263\n",
            "Epoch 74/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6489 - accuracy: 0.6215 - val_loss: 0.6865 - val_accuracy: 0.5263\n",
            "Epoch 75/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6478 - accuracy: 0.6163 - val_loss: 0.6866 - val_accuracy: 0.5250\n",
            "Epoch 76/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6494 - accuracy: 0.5998 - val_loss: 0.6894 - val_accuracy: 0.5387\n",
            "Epoch 77/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6523 - accuracy: 0.6004 - val_loss: 0.6933 - val_accuracy: 0.5400\n",
            "Epoch 78/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6465 - accuracy: 0.6101 - val_loss: 0.6881 - val_accuracy: 0.5412\n",
            "Epoch 79/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6488 - accuracy: 0.6239 - val_loss: 0.6870 - val_accuracy: 0.5375\n",
            "Epoch 80/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6516 - accuracy: 0.6126 - val_loss: 0.6909 - val_accuracy: 0.5362\n",
            "Epoch 81/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6466 - accuracy: 0.6228 - val_loss: 0.6894 - val_accuracy: 0.5375\n",
            "Epoch 82/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6451 - accuracy: 0.6130 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
            "Epoch 83/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6410 - accuracy: 0.6094 - val_loss: 0.7110 - val_accuracy: 0.5537\n",
            "Epoch 84/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6530 - accuracy: 0.6069 - val_loss: 0.6894 - val_accuracy: 0.5300\n",
            "Epoch 85/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6454 - accuracy: 0.6234 - val_loss: 0.6876 - val_accuracy: 0.5437\n",
            "Epoch 86/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6493 - accuracy: 0.6093 - val_loss: 0.6884 - val_accuracy: 0.5362\n",
            "Epoch 87/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6511 - accuracy: 0.6183 - val_loss: 0.6866 - val_accuracy: 0.5275\n",
            "Epoch 88/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6543 - accuracy: 0.6113 - val_loss: 0.6871 - val_accuracy: 0.5387\n",
            "Epoch 89/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6540 - accuracy: 0.6025 - val_loss: 0.6864 - val_accuracy: 0.5263\n",
            "Epoch 90/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6465 - accuracy: 0.6121 - val_loss: 0.6975 - val_accuracy: 0.5412\n",
            "Epoch 91/150\n",
            "38/38 [==============================] - 17s 454ms/step - loss: 0.6516 - accuracy: 0.6060 - val_loss: 0.6923 - val_accuracy: 0.5387\n",
            "Epoch 92/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6557 - accuracy: 0.6027 - val_loss: 0.6897 - val_accuracy: 0.5288\n",
            "Epoch 93/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6482 - accuracy: 0.6200 - val_loss: 0.6871 - val_accuracy: 0.5375\n",
            "Epoch 94/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6495 - accuracy: 0.6178 - val_loss: 0.6872 - val_accuracy: 0.5375\n",
            "Epoch 95/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6474 - accuracy: 0.6125 - val_loss: 0.6917 - val_accuracy: 0.5337\n",
            "Epoch 96/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6422 - accuracy: 0.6223 - val_loss: 0.6868 - val_accuracy: 0.5362\n",
            "Epoch 97/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6525 - accuracy: 0.6232 - val_loss: 0.6874 - val_accuracy: 0.5375\n",
            "Epoch 98/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6391 - accuracy: 0.6342 - val_loss: 0.6862 - val_accuracy: 0.5288\n",
            "Epoch 99/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6424 - accuracy: 0.6203 - val_loss: 0.6864 - val_accuracy: 0.5325\n",
            "Epoch 100/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6470 - accuracy: 0.6239 - val_loss: 0.6879 - val_accuracy: 0.5337\n",
            "Epoch 101/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6443 - accuracy: 0.6287 - val_loss: 0.6889 - val_accuracy: 0.5312\n",
            "Epoch 102/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6419 - accuracy: 0.6205 - val_loss: 0.6942 - val_accuracy: 0.5412\n",
            "Epoch 103/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6415 - accuracy: 0.6137 - val_loss: 0.6887 - val_accuracy: 0.5325\n",
            "Epoch 104/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6419 - accuracy: 0.6402 - val_loss: 0.6870 - val_accuracy: 0.5362\n",
            "Epoch 105/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6467 - accuracy: 0.6336 - val_loss: 0.6906 - val_accuracy: 0.5312\n",
            "Epoch 106/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6469 - accuracy: 0.6224 - val_loss: 0.6860 - val_accuracy: 0.5387\n",
            "Epoch 107/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6479 - accuracy: 0.6232 - val_loss: 0.6869 - val_accuracy: 0.5375\n",
            "Epoch 108/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6403 - accuracy: 0.6333 - val_loss: 0.6861 - val_accuracy: 0.5387\n",
            "Epoch 109/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6488 - accuracy: 0.6225 - val_loss: 0.6877 - val_accuracy: 0.5350\n",
            "Epoch 110/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6448 - accuracy: 0.6306 - val_loss: 0.6895 - val_accuracy: 0.5275\n",
            "Epoch 111/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6419 - accuracy: 0.6197 - val_loss: 0.6887 - val_accuracy: 0.5425\n",
            "Epoch 112/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6401 - accuracy: 0.6137 - val_loss: 0.6860 - val_accuracy: 0.5450\n",
            "Epoch 113/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6433 - accuracy: 0.6189 - val_loss: 0.6871 - val_accuracy: 0.5400\n",
            "Epoch 114/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6451 - accuracy: 0.6346 - val_loss: 0.6875 - val_accuracy: 0.5437\n",
            "Epoch 115/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6438 - accuracy: 0.6208 - val_loss: 0.6884 - val_accuracy: 0.5250\n",
            "Epoch 116/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6332 - accuracy: 0.6402 - val_loss: 0.6969 - val_accuracy: 0.5400\n",
            "Epoch 117/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6450 - accuracy: 0.6320 - val_loss: 0.6865 - val_accuracy: 0.5362\n",
            "Epoch 118/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6481 - accuracy: 0.6211 - val_loss: 0.6862 - val_accuracy: 0.5375\n",
            "Epoch 119/150\n",
            "38/38 [==============================] - 17s 454ms/step - loss: 0.6483 - accuracy: 0.6115 - val_loss: 0.6859 - val_accuracy: 0.5450\n",
            "Epoch 120/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6406 - accuracy: 0.6314 - val_loss: 0.6859 - val_accuracy: 0.5412\n",
            "Epoch 121/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6421 - accuracy: 0.6035 - val_loss: 0.6859 - val_accuracy: 0.5462\n",
            "Epoch 122/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6471 - accuracy: 0.6219 - val_loss: 0.6857 - val_accuracy: 0.5400\n",
            "Epoch 123/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6510 - accuracy: 0.6088 - val_loss: 0.6913 - val_accuracy: 0.5387\n",
            "Epoch 124/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6329 - accuracy: 0.6499 - val_loss: 0.6866 - val_accuracy: 0.5350\n",
            "Epoch 125/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6423 - accuracy: 0.6185 - val_loss: 0.6863 - val_accuracy: 0.5350\n",
            "Epoch 126/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6387 - accuracy: 0.6358 - val_loss: 0.6858 - val_accuracy: 0.5437\n",
            "Epoch 127/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6466 - accuracy: 0.6406 - val_loss: 0.6964 - val_accuracy: 0.5387\n",
            "Epoch 128/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6495 - accuracy: 0.6044 - val_loss: 0.6873 - val_accuracy: 0.5275\n",
            "Epoch 129/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6452 - accuracy: 0.6140 - val_loss: 0.6865 - val_accuracy: 0.5300\n",
            "Epoch 130/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6407 - accuracy: 0.6375 - val_loss: 0.6873 - val_accuracy: 0.5437\n",
            "Epoch 131/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6391 - accuracy: 0.6340 - val_loss: 0.6884 - val_accuracy: 0.5312\n",
            "Epoch 132/150\n",
            "38/38 [==============================] - 17s 454ms/step - loss: 0.6366 - accuracy: 0.6437 - val_loss: 0.6864 - val_accuracy: 0.5312\n",
            "Epoch 133/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6408 - accuracy: 0.6304 - val_loss: 0.6926 - val_accuracy: 0.5375\n",
            "Epoch 134/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6434 - accuracy: 0.6160 - val_loss: 0.6869 - val_accuracy: 0.5312\n",
            "Epoch 135/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6383 - accuracy: 0.6390 - val_loss: 0.6870 - val_accuracy: 0.5312\n",
            "Epoch 136/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6400 - accuracy: 0.6361 - val_loss: 0.6868 - val_accuracy: 0.5412\n",
            "Epoch 137/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6395 - accuracy: 0.6371 - val_loss: 0.6859 - val_accuracy: 0.5425\n",
            "Epoch 138/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6418 - accuracy: 0.6366 - val_loss: 0.6877 - val_accuracy: 0.5350\n",
            "Epoch 139/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6394 - accuracy: 0.6403 - val_loss: 0.6878 - val_accuracy: 0.5362\n",
            "Epoch 140/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6420 - accuracy: 0.6362 - val_loss: 0.6854 - val_accuracy: 0.5462\n",
            "Epoch 141/150\n",
            "38/38 [==============================] - 17s 452ms/step - loss: 0.6386 - accuracy: 0.6328 - val_loss: 0.6854 - val_accuracy: 0.5437\n",
            "Epoch 142/150\n",
            "38/38 [==============================] - 17s 454ms/step - loss: 0.6417 - accuracy: 0.6265 - val_loss: 0.6854 - val_accuracy: 0.5437\n",
            "Epoch 143/150\n",
            "38/38 [==============================] - 17s 454ms/step - loss: 0.6477 - accuracy: 0.6247 - val_loss: 0.6923 - val_accuracy: 0.5375\n",
            "Epoch 144/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6390 - accuracy: 0.6247 - val_loss: 0.6855 - val_accuracy: 0.5462\n",
            "Epoch 145/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6360 - accuracy: 0.6296 - val_loss: 0.6854 - val_accuracy: 0.5425\n",
            "Epoch 146/150\n",
            "38/38 [==============================] - 17s 450ms/step - loss: 0.6514 - accuracy: 0.6172 - val_loss: 0.6883 - val_accuracy: 0.5387\n",
            "Epoch 147/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6283 - accuracy: 0.6584 - val_loss: 0.6930 - val_accuracy: 0.5337\n",
            "Epoch 148/150\n",
            "38/38 [==============================] - 17s 451ms/step - loss: 0.6417 - accuracy: 0.6275 - val_loss: 0.6855 - val_accuracy: 0.5450\n",
            "Epoch 149/150\n",
            "38/38 [==============================] - 17s 453ms/step - loss: 0.6367 - accuracy: 0.6239 - val_loss: 0.6861 - val_accuracy: 0.5300\n",
            "Epoch 150/150\n",
            "38/38 [==============================] - 17s 454ms/step - loss: 0.6350 - accuracy: 0.6467 - val_loss: 0.6858 - val_accuracy: 0.5400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpkgwrgFpbJV"
      },
      "source": [
        "import pandas as pd\n",
        "Model_History = pd.DataFrame(history.history) \n",
        "\n",
        "hist_csv_file = '/content/drive/MyDrive/####Online_Journal/history_InceptionResNetV2_1.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    Model_History.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-rpLE-R28j"
      },
      "source": [
        "########################################  ResNet50 ###################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wKJ-fgAoORp"
      },
      "source": [
        "# ***ResNet50***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzASt2NVR3Kw"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yZ0LKFvShCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27883ec1-96d1-4375-f1a7-72f57eb593d3"
      },
      "source": [
        "num_classes = 2\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "model = ResNet50(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "#model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "  #print(layer ,layer.trainable)\n",
        "\n",
        "out = Dense(2, activation='softmax', name='output')(model.get_layer('avg_pool').output)\n",
        "ResNet50_1 = Model(image_input, out)\n",
        "\n",
        "ResNet50_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102973440/102967424 [==============================] - 1s 0us/step\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 2)            4098        avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELTTdVrYShFj",
        "outputId": "8eca1e2c-d77c-40d5-9b50-b22527f62be6"
      },
      "source": [
        "ResNet50_1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n",
        "history = ResNet50_1.fit(x_train, y_train, epochs=150, validation_data=(x_test, y_test), batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "38/38 [==============================] - 46s 289ms/step - loss: 0.8835 - accuracy: 0.4949 - val_loss: 0.6961 - val_accuracy: 0.5138\n",
            "Epoch 2/150\n",
            "38/38 [==============================] - 9s 238ms/step - loss: 0.6965 - accuracy: 0.4934 - val_loss: 0.6949 - val_accuracy: 0.4950\n",
            "Epoch 3/150\n",
            "38/38 [==============================] - 9s 238ms/step - loss: 0.6957 - accuracy: 0.4801 - val_loss: 0.6939 - val_accuracy: 0.5088\n",
            "Epoch 4/150\n",
            "38/38 [==============================] - 9s 240ms/step - loss: 0.6945 - accuracy: 0.4884 - val_loss: 0.6936 - val_accuracy: 0.5125\n",
            "Epoch 5/150\n",
            "38/38 [==============================] - 9s 242ms/step - loss: 0.6947 - accuracy: 0.4850 - val_loss: 0.6944 - val_accuracy: 0.5038\n",
            "Epoch 6/150\n",
            "38/38 [==============================] - 9s 244ms/step - loss: 0.6948 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5038\n",
            "Epoch 7/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6938 - accuracy: 0.4939 - val_loss: 0.6926 - val_accuracy: 0.4888\n",
            "Epoch 8/150\n",
            "38/38 [==============================] - 9s 251ms/step - loss: 0.6937 - accuracy: 0.5041 - val_loss: 0.6939 - val_accuracy: 0.5050\n",
            "Epoch 9/150\n",
            "38/38 [==============================] - 9s 252ms/step - loss: 0.6947 - accuracy: 0.4777 - val_loss: 0.6937 - val_accuracy: 0.5088\n",
            "Epoch 10/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6930 - accuracy: 0.5193 - val_loss: 0.6918 - val_accuracy: 0.5113\n",
            "Epoch 11/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6935 - accuracy: 0.4998 - val_loss: 0.6940 - val_accuracy: 0.5025\n",
            "Epoch 12/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6940 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.5088\n",
            "Epoch 13/150\n",
            "38/38 [==============================] - 9s 245ms/step - loss: 0.6928 - accuracy: 0.5111 - val_loss: 0.6913 - val_accuracy: 0.5050\n",
            "Epoch 14/150\n",
            "38/38 [==============================] - 9s 245ms/step - loss: 0.6917 - accuracy: 0.5099 - val_loss: 0.6927 - val_accuracy: 0.5113\n",
            "Epoch 15/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6929 - accuracy: 0.5249 - val_loss: 0.6909 - val_accuracy: 0.5138\n",
            "Epoch 16/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6913 - accuracy: 0.5316 - val_loss: 0.6911 - val_accuracy: 0.5400\n",
            "Epoch 17/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6922 - accuracy: 0.5033 - val_loss: 0.6919 - val_accuracy: 0.5200\n",
            "Epoch 18/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6901 - accuracy: 0.5498 - val_loss: 0.6900 - val_accuracy: 0.5425\n",
            "Epoch 19/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6910 - accuracy: 0.5336 - val_loss: 0.6902 - val_accuracy: 0.5325\n",
            "Epoch 20/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6900 - accuracy: 0.5456 - val_loss: 0.6909 - val_accuracy: 0.5425\n",
            "Epoch 21/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6897 - accuracy: 0.5410 - val_loss: 0.6899 - val_accuracy: 0.5125\n",
            "Epoch 22/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6915 - accuracy: 0.5131 - val_loss: 0.6898 - val_accuracy: 0.5475\n",
            "Epoch 23/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6911 - accuracy: 0.5304 - val_loss: 0.6901 - val_accuracy: 0.5400\n",
            "Epoch 24/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6897 - accuracy: 0.5488 - val_loss: 0.6889 - val_accuracy: 0.5300\n",
            "Epoch 25/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6890 - accuracy: 0.5403 - val_loss: 0.6890 - val_accuracy: 0.5325\n",
            "Epoch 26/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6896 - accuracy: 0.5495 - val_loss: 0.6889 - val_accuracy: 0.5387\n",
            "Epoch 27/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6888 - accuracy: 0.5472 - val_loss: 0.6881 - val_accuracy: 0.5612\n",
            "Epoch 28/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6894 - accuracy: 0.5235 - val_loss: 0.6884 - val_accuracy: 0.5387\n",
            "Epoch 29/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6877 - accuracy: 0.5663 - val_loss: 0.6877 - val_accuracy: 0.5425\n",
            "Epoch 30/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6869 - accuracy: 0.5359 - val_loss: 0.6913 - val_accuracy: 0.5300\n",
            "Epoch 31/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6873 - accuracy: 0.5510 - val_loss: 0.6873 - val_accuracy: 0.5450\n",
            "Epoch 32/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6890 - accuracy: 0.5169 - val_loss: 0.6881 - val_accuracy: 0.5362\n",
            "Epoch 33/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6878 - accuracy: 0.5561 - val_loss: 0.6873 - val_accuracy: 0.5475\n",
            "Epoch 34/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6862 - accuracy: 0.5311 - val_loss: 0.6873 - val_accuracy: 0.5487\n",
            "Epoch 35/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6871 - accuracy: 0.5665 - val_loss: 0.6866 - val_accuracy: 0.5437\n",
            "Epoch 36/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6861 - accuracy: 0.5604 - val_loss: 0.6882 - val_accuracy: 0.5387\n",
            "Epoch 37/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6842 - accuracy: 0.5743 - val_loss: 0.6879 - val_accuracy: 0.5088\n",
            "Epoch 38/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6895 - accuracy: 0.5050 - val_loss: 0.6866 - val_accuracy: 0.5462\n",
            "Epoch 39/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6845 - accuracy: 0.5682 - val_loss: 0.6862 - val_accuracy: 0.5550\n",
            "Epoch 40/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6843 - accuracy: 0.5752 - val_loss: 0.6869 - val_accuracy: 0.5362\n",
            "Epoch 41/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6861 - val_accuracy: 0.5288\n",
            "Epoch 42/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6856 - accuracy: 0.5322 - val_loss: 0.6868 - val_accuracy: 0.5375\n",
            "Epoch 43/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6865 - accuracy: 0.5646 - val_loss: 0.6854 - val_accuracy: 0.5487\n",
            "Epoch 44/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6854 - accuracy: 0.5373 - val_loss: 0.6904 - val_accuracy: 0.5362\n",
            "Epoch 45/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6842 - accuracy: 0.5688 - val_loss: 0.6850 - val_accuracy: 0.5487\n",
            "Epoch 46/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6837 - accuracy: 0.5443 - val_loss: 0.6862 - val_accuracy: 0.5412\n",
            "Epoch 47/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6845 - accuracy: 0.5578 - val_loss: 0.6848 - val_accuracy: 0.5487\n",
            "Epoch 48/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6862 - accuracy: 0.5399 - val_loss: 0.6849 - val_accuracy: 0.5550\n",
            "Epoch 49/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6855 - accuracy: 0.5367 - val_loss: 0.6874 - val_accuracy: 0.5425\n",
            "Epoch 50/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6816 - accuracy: 0.5603 - val_loss: 0.6861 - val_accuracy: 0.5325\n",
            "Epoch 51/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6853 - accuracy: 0.5658 - val_loss: 0.6844 - val_accuracy: 0.5525\n",
            "Epoch 52/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6832 - accuracy: 0.5582 - val_loss: 0.6843 - val_accuracy: 0.5537\n",
            "Epoch 53/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6839 - accuracy: 0.5561 - val_loss: 0.6839 - val_accuracy: 0.5537\n",
            "Epoch 54/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6827 - accuracy: 0.5485 - val_loss: 0.6849 - val_accuracy: 0.5437\n",
            "Epoch 55/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6835 - accuracy: 0.5618 - val_loss: 0.6843 - val_accuracy: 0.5450\n",
            "Epoch 56/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6829 - accuracy: 0.5647 - val_loss: 0.6839 - val_accuracy: 0.5450\n",
            "Epoch 57/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6825 - accuracy: 0.5358 - val_loss: 0.6840 - val_accuracy: 0.5475\n",
            "Epoch 58/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6812 - accuracy: 0.5778 - val_loss: 0.6833 - val_accuracy: 0.5500\n",
            "Epoch 59/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6832 - accuracy: 0.5415 - val_loss: 0.6840 - val_accuracy: 0.5462\n",
            "Epoch 60/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6810 - accuracy: 0.5750 - val_loss: 0.6830 - val_accuracy: 0.5500\n",
            "Epoch 61/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6830 - accuracy: 0.5440 - val_loss: 0.6860 - val_accuracy: 0.5387\n",
            "Epoch 62/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6826 - accuracy: 0.5572 - val_loss: 0.6833 - val_accuracy: 0.5537\n",
            "Epoch 63/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6814 - accuracy: 0.5533 - val_loss: 0.6840 - val_accuracy: 0.5475\n",
            "Epoch 64/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6829 - accuracy: 0.5491 - val_loss: 0.6826 - val_accuracy: 0.5462\n",
            "Epoch 65/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6807 - accuracy: 0.5532 - val_loss: 0.6828 - val_accuracy: 0.5537\n",
            "Epoch 66/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6785 - accuracy: 0.5729 - val_loss: 0.6829 - val_accuracy: 0.5475\n",
            "Epoch 67/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6801 - accuracy: 0.5758 - val_loss: 0.6845 - val_accuracy: 0.5425\n",
            "Epoch 68/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6801 - accuracy: 0.5755 - val_loss: 0.6829 - val_accuracy: 0.5450\n",
            "Epoch 69/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6839 - accuracy: 0.5615 - val_loss: 0.6821 - val_accuracy: 0.5500\n",
            "Epoch 70/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6801 - accuracy: 0.5494 - val_loss: 0.6827 - val_accuracy: 0.5475\n",
            "Epoch 71/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6799 - accuracy: 0.5624 - val_loss: 0.6819 - val_accuracy: 0.5525\n",
            "Epoch 72/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6796 - accuracy: 0.5657 - val_loss: 0.6825 - val_accuracy: 0.5500\n",
            "Epoch 73/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6821 - accuracy: 0.5336 - val_loss: 0.6834 - val_accuracy: 0.5500\n",
            "Epoch 74/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6777 - accuracy: 0.5788 - val_loss: 0.6839 - val_accuracy: 0.5500\n",
            "Epoch 75/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6835 - accuracy: 0.5558 - val_loss: 0.6839 - val_accuracy: 0.5500\n",
            "Epoch 76/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6783 - accuracy: 0.5833 - val_loss: 0.6818 - val_accuracy: 0.5500\n",
            "Epoch 77/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6772 - accuracy: 0.5842 - val_loss: 0.6818 - val_accuracy: 0.5462\n",
            "Epoch 78/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6817 - accuracy: 0.5420 - val_loss: 0.6831 - val_accuracy: 0.5525\n",
            "Epoch 79/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6798 - accuracy: 0.5667 - val_loss: 0.6814 - val_accuracy: 0.5475\n",
            "Epoch 80/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6813 - accuracy: 0.5585 - val_loss: 0.6815 - val_accuracy: 0.5487\n",
            "Epoch 81/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6770 - accuracy: 0.5772 - val_loss: 0.6814 - val_accuracy: 0.5537\n",
            "Epoch 82/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6780 - accuracy: 0.5963 - val_loss: 0.6810 - val_accuracy: 0.5500\n",
            "Epoch 83/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6796 - accuracy: 0.5606 - val_loss: 0.6833 - val_accuracy: 0.5500\n",
            "Epoch 84/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6767 - accuracy: 0.5933 - val_loss: 0.6808 - val_accuracy: 0.5562\n",
            "Epoch 85/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6801 - accuracy: 0.5529 - val_loss: 0.6807 - val_accuracy: 0.5500\n",
            "Epoch 86/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6781 - accuracy: 0.5488 - val_loss: 0.6814 - val_accuracy: 0.5475\n",
            "Epoch 87/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6809 - accuracy: 0.5682 - val_loss: 0.6820 - val_accuracy: 0.5500\n",
            "Epoch 88/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6781 - accuracy: 0.5629 - val_loss: 0.6811 - val_accuracy: 0.5500\n",
            "Epoch 89/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6788 - accuracy: 0.5690 - val_loss: 0.6808 - val_accuracy: 0.5487\n",
            "Epoch 90/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6776 - accuracy: 0.5621 - val_loss: 0.6809 - val_accuracy: 0.5462\n",
            "Epoch 91/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6799 - accuracy: 0.5756 - val_loss: 0.6806 - val_accuracy: 0.5500\n",
            "Epoch 92/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6780 - accuracy: 0.5671 - val_loss: 0.6805 - val_accuracy: 0.5512\n",
            "Epoch 93/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6822 - accuracy: 0.5370 - val_loss: 0.6809 - val_accuracy: 0.5500\n",
            "Epoch 94/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6789 - accuracy: 0.5633 - val_loss: 0.6804 - val_accuracy: 0.5525\n",
            "Epoch 95/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6742 - accuracy: 0.5621 - val_loss: 0.6818 - val_accuracy: 0.5525\n",
            "Epoch 96/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6766 - accuracy: 0.5714 - val_loss: 0.6799 - val_accuracy: 0.5550\n",
            "Epoch 97/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6795 - accuracy: 0.5600 - val_loss: 0.6806 - val_accuracy: 0.5500\n",
            "Epoch 98/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6759 - accuracy: 0.5669 - val_loss: 0.6819 - val_accuracy: 0.5550\n",
            "Epoch 99/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6763 - accuracy: 0.5698 - val_loss: 0.6805 - val_accuracy: 0.5487\n",
            "Epoch 100/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6779 - accuracy: 0.5824 - val_loss: 0.6798 - val_accuracy: 0.5625\n",
            "Epoch 101/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6771 - accuracy: 0.5591 - val_loss: 0.6796 - val_accuracy: 0.5462\n",
            "Epoch 102/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6756 - accuracy: 0.5734 - val_loss: 0.6797 - val_accuracy: 0.5437\n",
            "Epoch 103/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6737 - accuracy: 0.5718 - val_loss: 0.6818 - val_accuracy: 0.5562\n",
            "Epoch 104/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6791 - accuracy: 0.5564 - val_loss: 0.6824 - val_accuracy: 0.5487\n",
            "Epoch 105/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6798 - accuracy: 0.5624 - val_loss: 0.6796 - val_accuracy: 0.5450\n",
            "Epoch 106/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6767 - accuracy: 0.5788 - val_loss: 0.6795 - val_accuracy: 0.5625\n",
            "Epoch 107/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6762 - accuracy: 0.5667 - val_loss: 0.6798 - val_accuracy: 0.5487\n",
            "Epoch 108/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6775 - accuracy: 0.5611 - val_loss: 0.6822 - val_accuracy: 0.5500\n",
            "Epoch 109/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6743 - accuracy: 0.5916 - val_loss: 0.6796 - val_accuracy: 0.5425\n",
            "Epoch 110/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6749 - accuracy: 0.5788 - val_loss: 0.6792 - val_accuracy: 0.5512\n",
            "Epoch 111/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6750 - accuracy: 0.5637 - val_loss: 0.6795 - val_accuracy: 0.5425\n",
            "Epoch 112/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6729 - accuracy: 0.5778 - val_loss: 0.6801 - val_accuracy: 0.5525\n",
            "Epoch 113/150\n",
            "38/38 [==============================] - 9s 246ms/step - loss: 0.6778 - accuracy: 0.5785 - val_loss: 0.6798 - val_accuracy: 0.5475\n",
            "Epoch 114/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6776 - accuracy: 0.5675 - val_loss: 0.6806 - val_accuracy: 0.5537\n",
            "Epoch 115/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6731 - accuracy: 0.5832 - val_loss: 0.6793 - val_accuracy: 0.5400\n",
            "Epoch 116/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6757 - accuracy: 0.5578 - val_loss: 0.6825 - val_accuracy: 0.5500\n",
            "Epoch 117/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6755 - accuracy: 0.5812 - val_loss: 0.6790 - val_accuracy: 0.5500\n",
            "Epoch 118/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6756 - accuracy: 0.5659 - val_loss: 0.6794 - val_accuracy: 0.5425\n",
            "Epoch 119/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6743 - accuracy: 0.5703 - val_loss: 0.6796 - val_accuracy: 0.5462\n",
            "Epoch 120/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6755 - accuracy: 0.5639 - val_loss: 0.6792 - val_accuracy: 0.5437\n",
            "Epoch 121/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6747 - accuracy: 0.5729 - val_loss: 0.6797 - val_accuracy: 0.5462\n",
            "Epoch 122/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6736 - accuracy: 0.5775 - val_loss: 0.6787 - val_accuracy: 0.5475\n",
            "Epoch 123/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6765 - accuracy: 0.5742 - val_loss: 0.6786 - val_accuracy: 0.5500\n",
            "Epoch 124/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6724 - accuracy: 0.5592 - val_loss: 0.6800 - val_accuracy: 0.5587\n",
            "Epoch 125/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6756 - accuracy: 0.5684 - val_loss: 0.6803 - val_accuracy: 0.5537\n",
            "Epoch 126/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6761 - accuracy: 0.5711 - val_loss: 0.6786 - val_accuracy: 0.5487\n",
            "Epoch 127/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6718 - accuracy: 0.5779 - val_loss: 0.6798 - val_accuracy: 0.5550\n",
            "Epoch 128/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6726 - accuracy: 0.5806 - val_loss: 0.6785 - val_accuracy: 0.5575\n",
            "Epoch 129/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6741 - accuracy: 0.5597 - val_loss: 0.6796 - val_accuracy: 0.5525\n",
            "Epoch 130/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6756 - accuracy: 0.5493 - val_loss: 0.6830 - val_accuracy: 0.5462\n",
            "Epoch 131/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6799 - accuracy: 0.5581 - val_loss: 0.6802 - val_accuracy: 0.5525\n",
            "Epoch 132/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6743 - accuracy: 0.5847 - val_loss: 0.6787 - val_accuracy: 0.5400\n",
            "Epoch 133/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6710 - accuracy: 0.5837 - val_loss: 0.6785 - val_accuracy: 0.5500\n",
            "Epoch 134/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6737 - accuracy: 0.5722 - val_loss: 0.6799 - val_accuracy: 0.5575\n",
            "Epoch 135/150\n",
            "38/38 [==============================] - 9s 250ms/step - loss: 0.6739 - accuracy: 0.5871 - val_loss: 0.6784 - val_accuracy: 0.5638\n",
            "Epoch 136/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6748 - accuracy: 0.5617 - val_loss: 0.6782 - val_accuracy: 0.5462\n",
            "Epoch 137/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6748 - accuracy: 0.5599 - val_loss: 0.6814 - val_accuracy: 0.5537\n",
            "Epoch 138/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6783 - accuracy: 0.5445 - val_loss: 0.6872 - val_accuracy: 0.5525\n",
            "Epoch 139/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6730 - accuracy: 0.5714 - val_loss: 0.6794 - val_accuracy: 0.5550\n",
            "Epoch 140/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6724 - accuracy: 0.5867 - val_loss: 0.6782 - val_accuracy: 0.5650\n",
            "Epoch 141/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6762 - accuracy: 0.5621 - val_loss: 0.6782 - val_accuracy: 0.5500\n",
            "Epoch 142/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6749 - accuracy: 0.5619 - val_loss: 0.6792 - val_accuracy: 0.5537\n",
            "Epoch 143/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6724 - accuracy: 0.5818 - val_loss: 0.6780 - val_accuracy: 0.5462\n",
            "Epoch 144/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6766 - accuracy: 0.5661 - val_loss: 0.6782 - val_accuracy: 0.5650\n",
            "Epoch 145/150\n",
            "38/38 [==============================] - 9s 247ms/step - loss: 0.6805 - accuracy: 0.5372 - val_loss: 0.6816 - val_accuracy: 0.5575\n",
            "Epoch 146/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6740 - accuracy: 0.5857 - val_loss: 0.6789 - val_accuracy: 0.5487\n",
            "Epoch 147/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6718 - accuracy: 0.5726 - val_loss: 0.6785 - val_accuracy: 0.5425\n",
            "Epoch 148/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6728 - accuracy: 0.5707 - val_loss: 0.6778 - val_accuracy: 0.5537\n",
            "Epoch 149/150\n",
            "38/38 [==============================] - 9s 248ms/step - loss: 0.6710 - accuracy: 0.5625 - val_loss: 0.6786 - val_accuracy: 0.5450\n",
            "Epoch 150/150\n",
            "38/38 [==============================] - 9s 249ms/step - loss: 0.6712 - accuracy: 0.5782 - val_loss: 0.6778 - val_accuracy: 0.5487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AXsFLQ5TOf9"
      },
      "source": [
        "import pandas as pd\n",
        "Model_History = pd.DataFrame(history.history) \n",
        "\n",
        "hist_csv_file = '/content/drive/MyDrive/####Online_Journal/history_ResNet50_1.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    Model_History.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ4jBLCvoUHK"
      },
      "source": [
        "# ***ResNet101***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yxaIc0QYzYe"
      },
      "source": [
        "##################################### ResNet101 ############################################\n",
        "#keras.applications.ResNet152"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avLCn5wZYzcS"
      },
      "source": [
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e60Tci53YzgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76afcfd-749b-474c-ed96-d2c30c1ce5e7"
      },
      "source": [
        "num_classes = 2\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "model = MobileNet(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "#model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "  #print(layer ,layer.trainable)\n",
        "\n",
        "out = Dense(2, activation='softmax', name='output')(model.get_layer('reshape_2').output)\n",
        "MobileNet_1 = Model(image_input, out)\n",
        "\n",
        "MobileNet_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 2)                 2002      \n",
            "=================================================================\n",
            "Total params: 4,255,866\n",
            "Trainable params: 2,002\n",
            "Non-trainable params: 4,253,864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3NfHZcXZC98",
        "outputId": "f54b0a60-7191-492d-98a4-01fe73246ee6"
      },
      "source": [
        "MobileNet_1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n",
        "history = MobileNet_1.fit(x_train, y_train, epochs=150, validation_data=(x_test, y_test), batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "38/38 [==============================] - 6s 119ms/step - loss: 5.0737 - accuracy: 0.5195 - val_loss: 0.8001 - val_accuracy: 0.5113\n",
            "Epoch 2/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.8589 - accuracy: 0.4821 - val_loss: 0.7870 - val_accuracy: 0.5175\n",
            "Epoch 3/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.7774 - accuracy: 0.5010 - val_loss: 0.7442 - val_accuracy: 0.5275\n",
            "Epoch 4/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.7696 - accuracy: 0.5060 - val_loss: 0.7299 - val_accuracy: 0.5312\n",
            "Epoch 5/150\n",
            "38/38 [==============================] - 3s 93ms/step - loss: 0.7494 - accuracy: 0.5071 - val_loss: 0.7190 - val_accuracy: 0.5412\n",
            "Epoch 6/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.7359 - accuracy: 0.5248 - val_loss: 0.7126 - val_accuracy: 0.5462\n",
            "Epoch 7/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.7280 - accuracy: 0.5246 - val_loss: 0.7080 - val_accuracy: 0.5412\n",
            "Epoch 8/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.7256 - accuracy: 0.5336 - val_loss: 0.7012 - val_accuracy: 0.5750\n",
            "Epoch 9/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.7189 - accuracy: 0.5471 - val_loss: 0.7054 - val_accuracy: 0.5512\n",
            "Epoch 10/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.7071 - accuracy: 0.5444 - val_loss: 0.6929 - val_accuracy: 0.5725\n",
            "Epoch 11/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.7251 - accuracy: 0.5460 - val_loss: 0.6897 - val_accuracy: 0.5688\n",
            "Epoch 12/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.7080 - accuracy: 0.5475 - val_loss: 0.6873 - val_accuracy: 0.5675\n",
            "Epoch 13/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.7030 - accuracy: 0.5491 - val_loss: 0.6847 - val_accuracy: 0.5650\n",
            "Epoch 14/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6902 - accuracy: 0.5766 - val_loss: 0.6866 - val_accuracy: 0.5813\n",
            "Epoch 15/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6971 - accuracy: 0.5665 - val_loss: 0.6835 - val_accuracy: 0.5838\n",
            "Epoch 16/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.7005 - accuracy: 0.5442 - val_loss: 0.6792 - val_accuracy: 0.5688\n",
            "Epoch 17/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6848 - accuracy: 0.5726 - val_loss: 0.6802 - val_accuracy: 0.5788\n",
            "Epoch 18/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6987 - accuracy: 0.5555 - val_loss: 0.6767 - val_accuracy: 0.5663\n",
            "Epoch 19/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6879 - accuracy: 0.5606 - val_loss: 0.6767 - val_accuracy: 0.5738\n",
            "Epoch 20/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6927 - accuracy: 0.5537 - val_loss: 0.6922 - val_accuracy: 0.5800\n",
            "Epoch 21/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6878 - accuracy: 0.5695 - val_loss: 0.6743 - val_accuracy: 0.5725\n",
            "Epoch 22/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6818 - accuracy: 0.5693 - val_loss: 0.6722 - val_accuracy: 0.5800\n",
            "Epoch 23/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6836 - accuracy: 0.5562 - val_loss: 0.6720 - val_accuracy: 0.5788\n",
            "Epoch 24/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6757 - accuracy: 0.5905 - val_loss: 0.6712 - val_accuracy: 0.5800\n",
            "Epoch 25/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6777 - accuracy: 0.5781 - val_loss: 0.6735 - val_accuracy: 0.5775\n",
            "Epoch 26/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6808 - accuracy: 0.5695 - val_loss: 0.6737 - val_accuracy: 0.5850\n",
            "Epoch 27/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6839 - accuracy: 0.5733 - val_loss: 0.6708 - val_accuracy: 0.5763\n",
            "Epoch 28/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6895 - accuracy: 0.5652 - val_loss: 0.6696 - val_accuracy: 0.5775\n",
            "Epoch 29/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6826 - accuracy: 0.5738 - val_loss: 0.6710 - val_accuracy: 0.5813\n",
            "Epoch 30/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6775 - accuracy: 0.5654 - val_loss: 0.6712 - val_accuracy: 0.5800\n",
            "Epoch 31/150\n",
            "38/38 [==============================] - 3s 93ms/step - loss: 0.6731 - accuracy: 0.5807 - val_loss: 0.6682 - val_accuracy: 0.5775\n",
            "Epoch 32/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6770 - accuracy: 0.5706 - val_loss: 0.6698 - val_accuracy: 0.5875\n",
            "Epoch 33/150\n",
            "38/38 [==============================] - 3s 93ms/step - loss: 0.6654 - accuracy: 0.5958 - val_loss: 0.6673 - val_accuracy: 0.5850\n",
            "Epoch 34/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6721 - accuracy: 0.5884 - val_loss: 0.6736 - val_accuracy: 0.5825\n",
            "Epoch 35/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6708 - accuracy: 0.5764 - val_loss: 0.6695 - val_accuracy: 0.5850\n",
            "Epoch 36/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6644 - accuracy: 0.5720 - val_loss: 0.6776 - val_accuracy: 0.5838\n",
            "Epoch 37/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6858 - accuracy: 0.5709 - val_loss: 0.6867 - val_accuracy: 0.5888\n",
            "Epoch 38/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6652 - accuracy: 0.6037 - val_loss: 0.6679 - val_accuracy: 0.5925\n",
            "Epoch 39/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.6619 - accuracy: 0.5997 - val_loss: 0.6661 - val_accuracy: 0.5863\n",
            "Epoch 40/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6652 - accuracy: 0.5849 - val_loss: 0.6657 - val_accuracy: 0.5950\n",
            "Epoch 41/150\n",
            "38/38 [==============================] - 3s 93ms/step - loss: 0.6683 - accuracy: 0.5889 - val_loss: 0.6667 - val_accuracy: 0.5838\n",
            "Epoch 42/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6606 - accuracy: 0.5955 - val_loss: 0.6660 - val_accuracy: 0.5950\n",
            "Epoch 43/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6679 - accuracy: 0.5886 - val_loss: 0.6701 - val_accuracy: 0.5763\n",
            "Epoch 44/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6582 - accuracy: 0.6021 - val_loss: 0.6662 - val_accuracy: 0.5850\n",
            "Epoch 45/150\n",
            "38/38 [==============================] - 3s 93ms/step - loss: 0.6587 - accuracy: 0.5943 - val_loss: 0.6804 - val_accuracy: 0.5838\n",
            "Epoch 46/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6665 - accuracy: 0.5918 - val_loss: 0.6762 - val_accuracy: 0.5913\n",
            "Epoch 47/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6582 - accuracy: 0.6019 - val_loss: 0.6719 - val_accuracy: 0.5788\n",
            "Epoch 48/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6529 - accuracy: 0.6141 - val_loss: 0.6665 - val_accuracy: 0.5825\n",
            "Epoch 49/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6586 - accuracy: 0.6010 - val_loss: 0.6657 - val_accuracy: 0.5850\n",
            "Epoch 50/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6590 - accuracy: 0.5929 - val_loss: 0.6665 - val_accuracy: 0.5913\n",
            "Epoch 51/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6598 - accuracy: 0.6082 - val_loss: 0.6648 - val_accuracy: 0.6012\n",
            "Epoch 52/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6716 - accuracy: 0.5944 - val_loss: 0.6654 - val_accuracy: 0.5962\n",
            "Epoch 53/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6516 - accuracy: 0.6087 - val_loss: 0.6746 - val_accuracy: 0.5763\n",
            "Epoch 54/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6556 - accuracy: 0.6120 - val_loss: 0.6679 - val_accuracy: 0.5888\n",
            "Epoch 55/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6667 - accuracy: 0.5881 - val_loss: 0.6767 - val_accuracy: 0.5713\n",
            "Epoch 56/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6538 - accuracy: 0.6134 - val_loss: 0.6667 - val_accuracy: 0.5888\n",
            "Epoch 57/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6546 - accuracy: 0.5983 - val_loss: 0.6673 - val_accuracy: 0.5825\n",
            "Epoch 58/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6531 - accuracy: 0.6098 - val_loss: 0.6681 - val_accuracy: 0.5850\n",
            "Epoch 59/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6665 - accuracy: 0.6107 - val_loss: 0.6645 - val_accuracy: 0.5875\n",
            "Epoch 60/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6565 - accuracy: 0.6111 - val_loss: 0.6678 - val_accuracy: 0.5838\n",
            "Epoch 61/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6484 - accuracy: 0.6189 - val_loss: 0.6688 - val_accuracy: 0.5825\n",
            "Epoch 62/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6389 - accuracy: 0.6196 - val_loss: 0.6645 - val_accuracy: 0.5850\n",
            "Epoch 63/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.6559 - accuracy: 0.6136 - val_loss: 0.6666 - val_accuracy: 0.5900\n",
            "Epoch 64/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6525 - accuracy: 0.6040 - val_loss: 0.6650 - val_accuracy: 0.6037\n",
            "Epoch 65/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6507 - accuracy: 0.6182 - val_loss: 0.6651 - val_accuracy: 0.5838\n",
            "Epoch 66/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6543 - accuracy: 0.6062 - val_loss: 0.6859 - val_accuracy: 0.5838\n",
            "Epoch 67/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6550 - accuracy: 0.6003 - val_loss: 0.6664 - val_accuracy: 0.5888\n",
            "Epoch 68/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6478 - accuracy: 0.6174 - val_loss: 0.6651 - val_accuracy: 0.5975\n",
            "Epoch 69/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6469 - accuracy: 0.6166 - val_loss: 0.6667 - val_accuracy: 0.5900\n",
            "Epoch 70/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6422 - accuracy: 0.6078 - val_loss: 0.6644 - val_accuracy: 0.5888\n",
            "Epoch 71/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6365 - accuracy: 0.6314 - val_loss: 0.6702 - val_accuracy: 0.5813\n",
            "Epoch 72/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6485 - accuracy: 0.6134 - val_loss: 0.6666 - val_accuracy: 0.5900\n",
            "Epoch 73/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6527 - accuracy: 0.6130 - val_loss: 0.6750 - val_accuracy: 0.5725\n",
            "Epoch 74/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6481 - accuracy: 0.6177 - val_loss: 0.6698 - val_accuracy: 0.5913\n",
            "Epoch 75/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6407 - accuracy: 0.6221 - val_loss: 0.6849 - val_accuracy: 0.5875\n",
            "Epoch 76/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6493 - accuracy: 0.6164 - val_loss: 0.6645 - val_accuracy: 0.5938\n",
            "Epoch 77/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6428 - accuracy: 0.6103 - val_loss: 0.6679 - val_accuracy: 0.5838\n",
            "Epoch 78/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6391 - accuracy: 0.6257 - val_loss: 0.6698 - val_accuracy: 0.5825\n",
            "Epoch 79/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6423 - accuracy: 0.6235 - val_loss: 0.6670 - val_accuracy: 0.5925\n",
            "Epoch 80/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6559 - accuracy: 0.5977 - val_loss: 0.6650 - val_accuracy: 0.5950\n",
            "Epoch 81/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6440 - accuracy: 0.6154 - val_loss: 0.6714 - val_accuracy: 0.5825\n",
            "Epoch 82/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6434 - accuracy: 0.6172 - val_loss: 0.6648 - val_accuracy: 0.5938\n",
            "Epoch 83/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6375 - accuracy: 0.6258 - val_loss: 0.6642 - val_accuracy: 0.5900\n",
            "Epoch 84/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6526 - accuracy: 0.5994 - val_loss: 0.6738 - val_accuracy: 0.5713\n",
            "Epoch 85/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6521 - accuracy: 0.6239 - val_loss: 0.6640 - val_accuracy: 0.5913\n",
            "Epoch 86/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6398 - accuracy: 0.6359 - val_loss: 0.6768 - val_accuracy: 0.5925\n",
            "Epoch 87/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6508 - accuracy: 0.6116 - val_loss: 0.6721 - val_accuracy: 0.5913\n",
            "Epoch 88/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6313 - accuracy: 0.6348 - val_loss: 0.6712 - val_accuracy: 0.5838\n",
            "Epoch 89/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6527 - accuracy: 0.6135 - val_loss: 0.6651 - val_accuracy: 0.5987\n",
            "Epoch 90/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6509 - accuracy: 0.6260 - val_loss: 0.6646 - val_accuracy: 0.5925\n",
            "Epoch 91/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6409 - accuracy: 0.6226 - val_loss: 0.6654 - val_accuracy: 0.5938\n",
            "Epoch 92/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6317 - accuracy: 0.6379 - val_loss: 0.6676 - val_accuracy: 0.6000\n",
            "Epoch 93/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6515 - accuracy: 0.6158 - val_loss: 0.6641 - val_accuracy: 0.5875\n",
            "Epoch 94/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6453 - accuracy: 0.6257 - val_loss: 0.6965 - val_accuracy: 0.5788\n",
            "Epoch 95/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6453 - accuracy: 0.6130 - val_loss: 0.6871 - val_accuracy: 0.5688\n",
            "Epoch 96/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.6461 - accuracy: 0.6293 - val_loss: 0.6646 - val_accuracy: 0.5913\n",
            "Epoch 97/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6481 - accuracy: 0.6088 - val_loss: 0.6648 - val_accuracy: 0.5875\n",
            "Epoch 98/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6367 - accuracy: 0.6245 - val_loss: 0.6647 - val_accuracy: 0.5925\n",
            "Epoch 99/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6310 - accuracy: 0.6343 - val_loss: 0.6699 - val_accuracy: 0.5913\n",
            "Epoch 100/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6361 - accuracy: 0.6268 - val_loss: 0.6785 - val_accuracy: 0.5700\n",
            "Epoch 101/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6383 - accuracy: 0.6195 - val_loss: 0.6675 - val_accuracy: 0.5888\n",
            "Epoch 102/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6381 - accuracy: 0.6152 - val_loss: 0.6652 - val_accuracy: 0.6012\n",
            "Epoch 103/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6430 - accuracy: 0.6263 - val_loss: 0.6858 - val_accuracy: 0.5700\n",
            "Epoch 104/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6249 - accuracy: 0.6471 - val_loss: 0.6667 - val_accuracy: 0.6050\n",
            "Epoch 105/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6427 - accuracy: 0.6276 - val_loss: 0.6651 - val_accuracy: 0.6050\n",
            "Epoch 106/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6346 - accuracy: 0.6462 - val_loss: 0.6655 - val_accuracy: 0.5888\n",
            "Epoch 107/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6353 - accuracy: 0.6276 - val_loss: 0.6640 - val_accuracy: 0.5938\n",
            "Epoch 108/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6321 - accuracy: 0.6360 - val_loss: 0.6642 - val_accuracy: 0.5938\n",
            "Epoch 109/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6291 - accuracy: 0.6409 - val_loss: 0.6664 - val_accuracy: 0.5987\n",
            "Epoch 110/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6364 - accuracy: 0.6316 - val_loss: 0.6833 - val_accuracy: 0.5713\n",
            "Epoch 111/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6230 - accuracy: 0.6541 - val_loss: 0.6758 - val_accuracy: 0.5738\n",
            "Epoch 112/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6350 - accuracy: 0.6297 - val_loss: 0.6699 - val_accuracy: 0.5900\n",
            "Epoch 113/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6329 - accuracy: 0.6331 - val_loss: 0.6676 - val_accuracy: 0.5950\n",
            "Epoch 114/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6253 - accuracy: 0.6316 - val_loss: 0.6658 - val_accuracy: 0.5962\n",
            "Epoch 115/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6243 - accuracy: 0.6436 - val_loss: 0.6669 - val_accuracy: 0.5888\n",
            "Epoch 116/150\n",
            "38/38 [==============================] - 3s 92ms/step - loss: 0.6308 - accuracy: 0.6487 - val_loss: 0.6654 - val_accuracy: 0.6025\n",
            "Epoch 117/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6404 - accuracy: 0.6281 - val_loss: 0.6760 - val_accuracy: 0.5750\n",
            "Epoch 118/150\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.6346 - accuracy: 0.6253 - val_loss: 0.6641 - val_accuracy: 0.5938\n",
            "Epoch 119/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6404 - accuracy: 0.6250 - val_loss: 0.6647 - val_accuracy: 0.5925\n",
            "Epoch 120/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6382 - accuracy: 0.6278 - val_loss: 0.6682 - val_accuracy: 0.5888\n",
            "Epoch 121/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6424 - accuracy: 0.6192 - val_loss: 0.6712 - val_accuracy: 0.5900\n",
            "Epoch 122/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6233 - accuracy: 0.6436 - val_loss: 0.6673 - val_accuracy: 0.5925\n",
            "Epoch 123/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6330 - accuracy: 0.6408 - val_loss: 0.6921 - val_accuracy: 0.5700\n",
            "Epoch 124/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6348 - accuracy: 0.6162 - val_loss: 0.6640 - val_accuracy: 0.5938\n",
            "Epoch 125/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6332 - accuracy: 0.6334 - val_loss: 0.6670 - val_accuracy: 0.5962\n",
            "Epoch 126/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6168 - accuracy: 0.6473 - val_loss: 0.6644 - val_accuracy: 0.5987\n",
            "Epoch 127/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6349 - accuracy: 0.6324 - val_loss: 0.6666 - val_accuracy: 0.5938\n",
            "Epoch 128/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6192 - accuracy: 0.6424 - val_loss: 0.6748 - val_accuracy: 0.5925\n",
            "Epoch 129/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6578 - accuracy: 0.5989 - val_loss: 0.6644 - val_accuracy: 0.5913\n",
            "Epoch 130/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6264 - accuracy: 0.6343 - val_loss: 0.6686 - val_accuracy: 0.5888\n",
            "Epoch 131/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6243 - accuracy: 0.6347 - val_loss: 0.6750 - val_accuracy: 0.5775\n",
            "Epoch 132/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6280 - accuracy: 0.6393 - val_loss: 0.6716 - val_accuracy: 0.5888\n",
            "Epoch 133/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6301 - accuracy: 0.6377 - val_loss: 0.6643 - val_accuracy: 0.5950\n",
            "Epoch 134/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6268 - accuracy: 0.6447 - val_loss: 0.6649 - val_accuracy: 0.5938\n",
            "Epoch 135/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6251 - accuracy: 0.6330 - val_loss: 0.6654 - val_accuracy: 0.5987\n",
            "Epoch 136/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6257 - accuracy: 0.6485 - val_loss: 0.6649 - val_accuracy: 0.5962\n",
            "Epoch 137/150\n",
            "38/38 [==============================] - 4s 93ms/step - loss: 0.6308 - accuracy: 0.6250 - val_loss: 0.6646 - val_accuracy: 0.5987\n",
            "Epoch 138/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6164 - accuracy: 0.6501 - val_loss: 0.6754 - val_accuracy: 0.5863\n",
            "Epoch 139/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6419 - accuracy: 0.6327 - val_loss: 0.6642 - val_accuracy: 0.5987\n",
            "Epoch 140/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6214 - accuracy: 0.6424 - val_loss: 0.6637 - val_accuracy: 0.5925\n",
            "Epoch 141/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6273 - accuracy: 0.6418 - val_loss: 0.6676 - val_accuracy: 0.6025\n",
            "Epoch 142/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6252 - accuracy: 0.6504 - val_loss: 0.6662 - val_accuracy: 0.5962\n",
            "Epoch 143/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6355 - accuracy: 0.6269 - val_loss: 0.6669 - val_accuracy: 0.5975\n",
            "Epoch 144/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6186 - accuracy: 0.6467 - val_loss: 0.6665 - val_accuracy: 0.5938\n",
            "Epoch 145/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6302 - accuracy: 0.6274 - val_loss: 0.6643 - val_accuracy: 0.5938\n",
            "Epoch 146/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6209 - accuracy: 0.6398 - val_loss: 0.6738 - val_accuracy: 0.5900\n",
            "Epoch 147/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6228 - accuracy: 0.6428 - val_loss: 0.6661 - val_accuracy: 0.6000\n",
            "Epoch 148/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6257 - accuracy: 0.6406 - val_loss: 0.6649 - val_accuracy: 0.5938\n",
            "Epoch 149/150\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 0.6277 - accuracy: 0.6382 - val_loss: 0.6665 - val_accuracy: 0.5987\n",
            "Epoch 150/150\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.6231 - accuracy: 0.6459 - val_loss: 0.6646 - val_accuracy: 0.6025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxNVkGDHdhAY"
      },
      "source": [
        "import pandas as pd\n",
        "Model_History = pd.DataFrame(history.history) \n",
        "\n",
        "hist_csv_file = '/content/drive/MyDrive/####Online_Journal/history_MobileNet_1.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    Model_History.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evW_XJRSoYiD"
      },
      "source": [
        "# ***Xception***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq7NMI1fmy8E"
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "from keras.applications.xception import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndd68onunR3z",
        "outputId": "53baa1de-b73a-40e1-eae8-a4ef36e808d9"
      },
      "source": [
        "num_classes = 2\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "model = Xception(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "#model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "  #print(layer ,layer.trainable)\n",
        "\n",
        "out = Dense(2, activation='softmax', name='output')(model.get_layer('avg_pool').output)\n",
        "Xception_1 = Model(image_input, out)\n",
        "\n",
        "Xception_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels.h5\n",
            "91889664/91884032 [==============================] - 1s 0us/step\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 111, 111, 32) 864         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 111, 111, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 111, 111, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 109, 109, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 109, 109, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 109, 109, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 109, 109, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 109, 109, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 109, 109, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 55, 55, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 55, 55, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 55, 55, 128)  512         conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 55, 55, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 55, 55, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 55, 55, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 55, 55, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 55, 55, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 28, 28, 256)  32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 28, 28, 256)  1024        conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 28, 28, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 28, 28, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 28, 28, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 28, 28, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 28, 28, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 14, 14, 728)  186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 14, 14, 728)  2912        conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 14, 14, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 14, 14, 728)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 14, 14, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 14, 14, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 14, 14, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 14, 14, 728)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 14, 14, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 14, 14, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 14, 14, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 14, 14, 728)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 14, 14, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 14, 14, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 14, 14, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 14, 14, 728)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 14, 14, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 14, 14, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 14, 14, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 14, 14, 728)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 14, 14, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 14, 14, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 14, 14, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 14, 14, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 14, 14, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 14, 14, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 14, 14, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 14, 14, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 14, 14, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 14, 14, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 14, 14, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 14, 14, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 7, 7, 1024)   745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 7, 7, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 7, 7, 1024)   4096        conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 7, 7, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 7, 7, 1536)   1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 7, 7, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 7, 7, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 7, 7, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 7, 7, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 7, 7, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 2)            4098        avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 20,865,578\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 20,861,480\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGAJVtYnnUqx",
        "outputId": "cf88dc9d-dbce-460b-fb61-95be1c9eb654"
      },
      "source": [
        "Xception_1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n",
        "history = Xception_1.fit(x_train, y_train, epochs=150, validation_data=(x_test, y_test), batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "38/38 [==============================] - 50s 383ms/step - loss: 0.6920 - accuracy: 0.5172 - val_loss: 0.6891 - val_accuracy: 0.5437\n",
            "Epoch 2/150\n",
            "38/38 [==============================] - 13s 331ms/step - loss: 0.6911 - accuracy: 0.5365 - val_loss: 0.6828 - val_accuracy: 0.5512\n",
            "Epoch 3/150\n",
            "38/38 [==============================] - 13s 338ms/step - loss: 0.6797 - accuracy: 0.5650 - val_loss: 0.6802 - val_accuracy: 0.5575\n",
            "Epoch 4/150\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.6780 - accuracy: 0.5747 - val_loss: 0.6796 - val_accuracy: 0.5437\n",
            "Epoch 5/150\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.6812 - accuracy: 0.5598 - val_loss: 0.6772 - val_accuracy: 0.5550\n",
            "Epoch 6/150\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.6673 - accuracy: 0.5901 - val_loss: 0.6760 - val_accuracy: 0.5562\n",
            "Epoch 7/150\n",
            "38/38 [==============================] - 14s 359ms/step - loss: 0.6664 - accuracy: 0.5799 - val_loss: 0.6752 - val_accuracy: 0.5600\n",
            "Epoch 8/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6653 - accuracy: 0.5828 - val_loss: 0.6771 - val_accuracy: 0.5625\n",
            "Epoch 9/150\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.6690 - accuracy: 0.5750 - val_loss: 0.6743 - val_accuracy: 0.5575\n",
            "Epoch 10/150\n",
            "38/38 [==============================] - 14s 369ms/step - loss: 0.6711 - accuracy: 0.5805 - val_loss: 0.6729 - val_accuracy: 0.5625\n",
            "Epoch 11/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6694 - accuracy: 0.5735 - val_loss: 0.6736 - val_accuracy: 0.5612\n",
            "Epoch 12/150\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.6718 - accuracy: 0.5794 - val_loss: 0.6721 - val_accuracy: 0.5612\n",
            "Epoch 13/150\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.6617 - accuracy: 0.5906 - val_loss: 0.6737 - val_accuracy: 0.5638\n",
            "Epoch 14/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6686 - accuracy: 0.5688 - val_loss: 0.6729 - val_accuracy: 0.5625\n",
            "Epoch 15/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6621 - accuracy: 0.5958 - val_loss: 0.6751 - val_accuracy: 0.5638\n",
            "Epoch 16/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6606 - accuracy: 0.5950 - val_loss: 0.6719 - val_accuracy: 0.5587\n",
            "Epoch 17/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6653 - accuracy: 0.5720 - val_loss: 0.6713 - val_accuracy: 0.5600\n",
            "Epoch 18/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6569 - accuracy: 0.5872 - val_loss: 0.6734 - val_accuracy: 0.5638\n",
            "Epoch 19/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6634 - accuracy: 0.5820 - val_loss: 0.6722 - val_accuracy: 0.5638\n",
            "Epoch 20/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6536 - accuracy: 0.6010 - val_loss: 0.6719 - val_accuracy: 0.5612\n",
            "Epoch 21/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6561 - accuracy: 0.6132 - val_loss: 0.6723 - val_accuracy: 0.5612\n",
            "Epoch 22/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6571 - accuracy: 0.5964 - val_loss: 0.6708 - val_accuracy: 0.5612\n",
            "Epoch 23/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6609 - accuracy: 0.5701 - val_loss: 0.6713 - val_accuracy: 0.5700\n",
            "Epoch 24/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6481 - accuracy: 0.6035 - val_loss: 0.6708 - val_accuracy: 0.5638\n",
            "Epoch 25/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6565 - accuracy: 0.5988 - val_loss: 0.6710 - val_accuracy: 0.5562\n",
            "Epoch 26/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6585 - accuracy: 0.5900 - val_loss: 0.6709 - val_accuracy: 0.5638\n",
            "Epoch 27/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6603 - accuracy: 0.5870 - val_loss: 0.6723 - val_accuracy: 0.5725\n",
            "Epoch 28/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6468 - accuracy: 0.6133 - val_loss: 0.6710 - val_accuracy: 0.5625\n",
            "Epoch 29/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6574 - accuracy: 0.5960 - val_loss: 0.6725 - val_accuracy: 0.5725\n",
            "Epoch 30/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6512 - accuracy: 0.6074 - val_loss: 0.6712 - val_accuracy: 0.5663\n",
            "Epoch 31/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6522 - accuracy: 0.5938 - val_loss: 0.6733 - val_accuracy: 0.5750\n",
            "Epoch 32/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6540 - accuracy: 0.5984 - val_loss: 0.6716 - val_accuracy: 0.5688\n",
            "Epoch 33/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6530 - accuracy: 0.5851 - val_loss: 0.6728 - val_accuracy: 0.5750\n",
            "Epoch 34/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6518 - accuracy: 0.6072 - val_loss: 0.6706 - val_accuracy: 0.5650\n",
            "Epoch 35/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6436 - accuracy: 0.6157 - val_loss: 0.6711 - val_accuracy: 0.5738\n",
            "Epoch 36/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6461 - accuracy: 0.6172 - val_loss: 0.6705 - val_accuracy: 0.5675\n",
            "Epoch 37/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6460 - accuracy: 0.6153 - val_loss: 0.6720 - val_accuracy: 0.5750\n",
            "Epoch 38/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6505 - accuracy: 0.6136 - val_loss: 0.6705 - val_accuracy: 0.5688\n",
            "Epoch 39/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6534 - accuracy: 0.6075 - val_loss: 0.6726 - val_accuracy: 0.5750\n",
            "Epoch 40/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6587 - accuracy: 0.5875 - val_loss: 0.6722 - val_accuracy: 0.5725\n",
            "Epoch 41/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6486 - accuracy: 0.6144 - val_loss: 0.6727 - val_accuracy: 0.5738\n",
            "Epoch 42/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6618 - accuracy: 0.5810 - val_loss: 0.6698 - val_accuracy: 0.5750\n",
            "Epoch 43/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6434 - accuracy: 0.6250 - val_loss: 0.6724 - val_accuracy: 0.5725\n",
            "Epoch 44/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6541 - accuracy: 0.5852 - val_loss: 0.6739 - val_accuracy: 0.5788\n",
            "Epoch 45/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6436 - accuracy: 0.6239 - val_loss: 0.6723 - val_accuracy: 0.5738\n",
            "Epoch 46/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6483 - accuracy: 0.6149 - val_loss: 0.6708 - val_accuracy: 0.5788\n",
            "Epoch 47/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6470 - accuracy: 0.6051 - val_loss: 0.6715 - val_accuracy: 0.5738\n",
            "Epoch 48/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6484 - accuracy: 0.6102 - val_loss: 0.6699 - val_accuracy: 0.5763\n",
            "Epoch 49/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6448 - accuracy: 0.6125 - val_loss: 0.6719 - val_accuracy: 0.5775\n",
            "Epoch 50/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6379 - accuracy: 0.6255 - val_loss: 0.6709 - val_accuracy: 0.5813\n",
            "Epoch 51/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6540 - accuracy: 0.6025 - val_loss: 0.6742 - val_accuracy: 0.5825\n",
            "Epoch 52/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6492 - accuracy: 0.6119 - val_loss: 0.6703 - val_accuracy: 0.5738\n",
            "Epoch 53/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6508 - accuracy: 0.6171 - val_loss: 0.6747 - val_accuracy: 0.5863\n",
            "Epoch 54/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6466 - accuracy: 0.6111 - val_loss: 0.6699 - val_accuracy: 0.5763\n",
            "Epoch 55/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6486 - accuracy: 0.6116 - val_loss: 0.6744 - val_accuracy: 0.5863\n",
            "Epoch 56/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6514 - accuracy: 0.6084 - val_loss: 0.6743 - val_accuracy: 0.5850\n",
            "Epoch 57/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6432 - accuracy: 0.6105 - val_loss: 0.6717 - val_accuracy: 0.5763\n",
            "Epoch 58/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6385 - accuracy: 0.6251 - val_loss: 0.6699 - val_accuracy: 0.5713\n",
            "Epoch 59/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6430 - accuracy: 0.6177 - val_loss: 0.6702 - val_accuracy: 0.5738\n",
            "Epoch 60/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6478 - accuracy: 0.6151 - val_loss: 0.6698 - val_accuracy: 0.5850\n",
            "Epoch 61/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6370 - accuracy: 0.6169 - val_loss: 0.6699 - val_accuracy: 0.5838\n",
            "Epoch 62/150\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6498 - accuracy: 0.6032 - val_loss: 0.6698 - val_accuracy: 0.5738\n",
            "Epoch 63/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6525 - accuracy: 0.6051 - val_loss: 0.6697 - val_accuracy: 0.5800\n",
            "Epoch 64/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6416 - accuracy: 0.6273 - val_loss: 0.6700 - val_accuracy: 0.5838\n",
            "Epoch 65/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6359 - accuracy: 0.6214 - val_loss: 0.6725 - val_accuracy: 0.5775\n",
            "Epoch 66/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6423 - accuracy: 0.6057 - val_loss: 0.6708 - val_accuracy: 0.5838\n",
            "Epoch 67/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6469 - accuracy: 0.6064 - val_loss: 0.6697 - val_accuracy: 0.5788\n",
            "Epoch 68/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6476 - accuracy: 0.6078 - val_loss: 0.6721 - val_accuracy: 0.5800\n",
            "Epoch 69/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6448 - accuracy: 0.6026 - val_loss: 0.6698 - val_accuracy: 0.5788\n",
            "Epoch 70/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6386 - accuracy: 0.6327 - val_loss: 0.6707 - val_accuracy: 0.5838\n",
            "Epoch 71/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6420 - accuracy: 0.6119 - val_loss: 0.6754 - val_accuracy: 0.5850\n",
            "Epoch 72/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6376 - accuracy: 0.6190 - val_loss: 0.6719 - val_accuracy: 0.5813\n",
            "Epoch 73/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6395 - accuracy: 0.6251 - val_loss: 0.6697 - val_accuracy: 0.5800\n",
            "Epoch 74/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6396 - accuracy: 0.6153 - val_loss: 0.6726 - val_accuracy: 0.5763\n",
            "Epoch 75/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6433 - accuracy: 0.6131 - val_loss: 0.6696 - val_accuracy: 0.5800\n",
            "Epoch 76/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6435 - accuracy: 0.6227 - val_loss: 0.6696 - val_accuracy: 0.5788\n",
            "Epoch 77/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6411 - accuracy: 0.6142 - val_loss: 0.6698 - val_accuracy: 0.5825\n",
            "Epoch 78/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6399 - accuracy: 0.6269 - val_loss: 0.6698 - val_accuracy: 0.5813\n",
            "Epoch 79/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6345 - accuracy: 0.6335 - val_loss: 0.6698 - val_accuracy: 0.5775\n",
            "Epoch 80/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6371 - accuracy: 0.6246 - val_loss: 0.6703 - val_accuracy: 0.5725\n",
            "Epoch 81/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6448 - accuracy: 0.6050 - val_loss: 0.6697 - val_accuracy: 0.5813\n",
            "Epoch 82/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6365 - accuracy: 0.6145 - val_loss: 0.6701 - val_accuracy: 0.5850\n",
            "Epoch 83/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6387 - accuracy: 0.6226 - val_loss: 0.6698 - val_accuracy: 0.5825\n",
            "Epoch 84/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6388 - accuracy: 0.6284 - val_loss: 0.6702 - val_accuracy: 0.5825\n",
            "Epoch 85/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6394 - accuracy: 0.6314 - val_loss: 0.6711 - val_accuracy: 0.5838\n",
            "Epoch 86/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6433 - accuracy: 0.6160 - val_loss: 0.6722 - val_accuracy: 0.5838\n",
            "Epoch 87/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6299 - accuracy: 0.6369 - val_loss: 0.6708 - val_accuracy: 0.5838\n",
            "Epoch 88/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6424 - accuracy: 0.6185 - val_loss: 0.6699 - val_accuracy: 0.5838\n",
            "Epoch 89/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6398 - accuracy: 0.6206 - val_loss: 0.6719 - val_accuracy: 0.5838\n",
            "Epoch 90/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6326 - accuracy: 0.6234 - val_loss: 0.6786 - val_accuracy: 0.5763\n",
            "Epoch 91/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6331 - accuracy: 0.6286 - val_loss: 0.6697 - val_accuracy: 0.5725\n",
            "Epoch 92/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6345 - accuracy: 0.6279 - val_loss: 0.6727 - val_accuracy: 0.5825\n",
            "Epoch 93/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6368 - accuracy: 0.6259 - val_loss: 0.6702 - val_accuracy: 0.5763\n",
            "Epoch 94/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6360 - accuracy: 0.6299 - val_loss: 0.6705 - val_accuracy: 0.5738\n",
            "Epoch 95/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6350 - accuracy: 0.6200 - val_loss: 0.6750 - val_accuracy: 0.5875\n",
            "Epoch 96/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6341 - accuracy: 0.6298 - val_loss: 0.6703 - val_accuracy: 0.5813\n",
            "Epoch 97/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6399 - accuracy: 0.6263 - val_loss: 0.6700 - val_accuracy: 0.5863\n",
            "Epoch 98/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6334 - accuracy: 0.6317 - val_loss: 0.6705 - val_accuracy: 0.5825\n",
            "Epoch 99/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6271 - accuracy: 0.6284 - val_loss: 0.6724 - val_accuracy: 0.5825\n",
            "Epoch 100/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6345 - accuracy: 0.6394 - val_loss: 0.6702 - val_accuracy: 0.5838\n",
            "Epoch 101/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6314 - accuracy: 0.6283 - val_loss: 0.6698 - val_accuracy: 0.5750\n",
            "Epoch 102/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6340 - accuracy: 0.6283 - val_loss: 0.6712 - val_accuracy: 0.5838\n",
            "Epoch 103/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6309 - accuracy: 0.6391 - val_loss: 0.6705 - val_accuracy: 0.5775\n",
            "Epoch 104/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6263 - accuracy: 0.6350 - val_loss: 0.6702 - val_accuracy: 0.5825\n",
            "Epoch 105/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6314 - accuracy: 0.6404 - val_loss: 0.6699 - val_accuracy: 0.5825\n",
            "Epoch 106/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6332 - accuracy: 0.6263 - val_loss: 0.6716 - val_accuracy: 0.5863\n",
            "Epoch 107/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6369 - accuracy: 0.6335 - val_loss: 0.6744 - val_accuracy: 0.5800\n",
            "Epoch 108/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6294 - accuracy: 0.6282 - val_loss: 0.6701 - val_accuracy: 0.5800\n",
            "Epoch 109/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6257 - accuracy: 0.6476 - val_loss: 0.6701 - val_accuracy: 0.5750\n",
            "Epoch 110/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6332 - accuracy: 0.6289 - val_loss: 0.6709 - val_accuracy: 0.5850\n",
            "Epoch 111/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6322 - accuracy: 0.6275 - val_loss: 0.6699 - val_accuracy: 0.5788\n",
            "Epoch 112/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6280 - accuracy: 0.6381 - val_loss: 0.6703 - val_accuracy: 0.5775\n",
            "Epoch 113/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6282 - accuracy: 0.6403 - val_loss: 0.6707 - val_accuracy: 0.5850\n",
            "Epoch 114/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6277 - accuracy: 0.6369 - val_loss: 0.6742 - val_accuracy: 0.5813\n",
            "Epoch 115/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6294 - accuracy: 0.6325 - val_loss: 0.6715 - val_accuracy: 0.5863\n",
            "Epoch 116/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6310 - accuracy: 0.6359 - val_loss: 0.6743 - val_accuracy: 0.5788\n",
            "Epoch 117/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6275 - accuracy: 0.6383 - val_loss: 0.6708 - val_accuracy: 0.5850\n",
            "Epoch 118/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6212 - accuracy: 0.6490 - val_loss: 0.6701 - val_accuracy: 0.5763\n",
            "Epoch 119/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6325 - accuracy: 0.6190 - val_loss: 0.6738 - val_accuracy: 0.5813\n",
            "Epoch 120/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6350 - accuracy: 0.6163 - val_loss: 0.6704 - val_accuracy: 0.5713\n",
            "Epoch 121/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6264 - accuracy: 0.6301 - val_loss: 0.6700 - val_accuracy: 0.5800\n",
            "Epoch 122/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6320 - accuracy: 0.6292 - val_loss: 0.6759 - val_accuracy: 0.5800\n",
            "Epoch 123/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6365 - accuracy: 0.6192 - val_loss: 0.6704 - val_accuracy: 0.5800\n",
            "Epoch 124/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6310 - accuracy: 0.6339 - val_loss: 0.6701 - val_accuracy: 0.5750\n",
            "Epoch 125/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6225 - accuracy: 0.6414 - val_loss: 0.6702 - val_accuracy: 0.5738\n",
            "Epoch 126/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6302 - accuracy: 0.6357 - val_loss: 0.6712 - val_accuracy: 0.5875\n",
            "Epoch 127/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6348 - accuracy: 0.6330 - val_loss: 0.6714 - val_accuracy: 0.5850\n",
            "Epoch 128/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6256 - accuracy: 0.6356 - val_loss: 0.6709 - val_accuracy: 0.5750\n",
            "Epoch 129/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6307 - accuracy: 0.6347 - val_loss: 0.6706 - val_accuracy: 0.5825\n",
            "Epoch 130/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6319 - accuracy: 0.6304 - val_loss: 0.6703 - val_accuracy: 0.5763\n",
            "Epoch 131/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6342 - accuracy: 0.6254 - val_loss: 0.6750 - val_accuracy: 0.5825\n",
            "Epoch 132/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6288 - accuracy: 0.6377 - val_loss: 0.6708 - val_accuracy: 0.5788\n",
            "Epoch 133/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6263 - accuracy: 0.6442 - val_loss: 0.6703 - val_accuracy: 0.5763\n",
            "Epoch 134/150\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.6279 - accuracy: 0.6405 - val_loss: 0.6707 - val_accuracy: 0.5788\n",
            "Epoch 135/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6225 - accuracy: 0.6461 - val_loss: 0.6704 - val_accuracy: 0.5763\n",
            "Epoch 136/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6354 - accuracy: 0.6304 - val_loss: 0.6703 - val_accuracy: 0.5725\n",
            "Epoch 137/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6247 - accuracy: 0.6417 - val_loss: 0.6706 - val_accuracy: 0.5750\n",
            "Epoch 138/150\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.6361 - accuracy: 0.6310 - val_loss: 0.6725 - val_accuracy: 0.5825\n",
            "Epoch 139/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6341 - accuracy: 0.6212 - val_loss: 0.6736 - val_accuracy: 0.5838\n",
            "Epoch 140/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6273 - accuracy: 0.6353 - val_loss: 0.6724 - val_accuracy: 0.5825\n",
            "Epoch 141/150\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.6290 - accuracy: 0.6320 - val_loss: 0.6710 - val_accuracy: 0.5788\n",
            "Epoch 142/150\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6258 - accuracy: 0.6408 - val_loss: 0.6705 - val_accuracy: 0.5738\n",
            "Epoch 143/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6238 - accuracy: 0.6368 - val_loss: 0.6708 - val_accuracy: 0.5750\n",
            "Epoch 144/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6311 - accuracy: 0.6293 - val_loss: 0.6705 - val_accuracy: 0.5725\n",
            "Epoch 145/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6209 - accuracy: 0.6374 - val_loss: 0.6723 - val_accuracy: 0.5813\n",
            "Epoch 146/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6190 - accuracy: 0.6439 - val_loss: 0.6704 - val_accuracy: 0.5775\n",
            "Epoch 147/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6254 - accuracy: 0.6275 - val_loss: 0.6706 - val_accuracy: 0.5725\n",
            "Epoch 148/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6284 - accuracy: 0.6285 - val_loss: 0.6708 - val_accuracy: 0.5800\n",
            "Epoch 149/150\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.6275 - accuracy: 0.6387 - val_loss: 0.6706 - val_accuracy: 0.5788\n",
            "Epoch 150/150\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6192 - accuracy: 0.6386 - val_loss: 0.6714 - val_accuracy: 0.5763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNuwB_brp8W-"
      },
      "source": [
        "import pandas as pd\n",
        "Model_History = pd.DataFrame(history.history) \n",
        "\n",
        "hist_csv_file = '/content/drive/MyDrive/####Online_Journal/history_Xception_1.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    Model_History.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}